{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/israinamdar493-ops/retinalprojectsizz4/blob/main/finalproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9r_at-dWvTp",
        "outputId": "27c1ad4c-6f2f-45ac-95fe-e5e56911e0ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/retinalnewproject\n",
            ".:\n",
            "datasets\n",
            "ensemble\n",
            "federated\n",
            "outputs\n",
            "preprocessing\n",
            "reports\n",
            "results\n",
            "segmentation\n",
            "\n",
            "./datasets:\n",
            "APTOS\n",
            "CHASEDB1\n",
            "DRIVE\n",
            "Messidor2\n",
            "ODIR5K\n",
            "RFMiD\n",
            "\n",
            "./datasets/APTOS:\n",
            "test.csv\n",
            "test_images\n",
            "train_1.csv\n",
            "train_images\n",
            "valid.csv\n",
            "val_images\n",
            "\n",
            "./datasets/APTOS/test_images:\n",
            "test_images\n",
            "\n",
            "./datasets/APTOS/test_images/test_images:\n",
            "e4dcca36ceb4.png\n",
            "e50b0174690d.png\n",
            "e5197d77ec68.png\n",
            "e529c5757d64.png\n",
            "e594c19e2e1d.png\n",
            "e5de79795c1d.png\n",
            "e60e4edb3ca9.png\n",
            "e6552b7432b3.png\n",
            "e66855a5c583.png\n",
            "e68746d426b2.png\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.19)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.12/dist-packages (2.0.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.34.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.6.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from albumentations) (1.16.1)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.11.7)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.12/dist-packages (from albumentations) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.12/dist-packages (from albumentations) (4.12.0.88)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (3.12.6)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (6.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.1.9)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.8.3)\n",
            "python3: can't open file '/content/drive/MyDrive/retinalnewproject/src/phase1_demo.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Go inside your Drive project folder\n",
        "%cd /content/drive/MyDrive/retinalnewproject\n",
        "\n",
        "# (Optional) check the structure\n",
        "!ls -R | head -40   # shows first 40 lines of folder tree\n",
        "\n",
        "# Install all dependencies\n",
        "!pip install torch torchvision timm albumentations pandas scikit-learn xgboost opencv-python matplotlib\n",
        "\n",
        "# >>> RUN YOUR SCRIPT (Phase-1 demo or real data) <<<\n",
        "# Replace with the actual file you want to run, for example:\n",
        "!python src/phase1_demo.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-0T5ar2EUdc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDh1V3Z9mjua"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PT_faPw3XTA-",
        "outputId": "5fdc5075-7b02-44b9-c812-35e9a6a2487e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ File created at /content/drive/MyDrive/retinalnewproject/phase1_demo.py\n"
          ]
        }
      ],
      "source": [
        "# Save this cell as a .py file inside your Drive\n",
        "demo_code = \"\"\"\n",
        "import torch, torchvision\n",
        "import pandas as pd\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "print('✅ Demo running with Torch version:', torch.__version__)\n",
        "\n",
        "# Define dataset paths - These should match the variables defined in the notebook\n",
        "train_csv = \"/content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_1.csv\"\n",
        "train_dir = \"/content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images\"\n",
        "val_csv = \"/content/drive/MyDrive/retinalnewproject/datasets/APTOS/valid.csv\"\n",
        "val_dir = \"/content/drive/MyDrive/retinalnewproject/datasets/APTOS/val_images\"\n",
        "\n",
        "\n",
        "class RetinalDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.df.iloc[idx, 0] + '.png')\n",
        "        image = Image.open(img_name)\n",
        "        label = self.df.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Define a simple transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "try:\n",
        "    train_dataset = RetinalDataset(train_csv, train_dir, transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    print(f\"✅ Loaded training dataset with {len(train_dataset)} images.\")\n",
        "\n",
        "    val_dataset = RetinalDataset(val_csv, val_dir, transform)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "    print(f\"✅ Loaded validation dataset with {len(val_dataset)} images.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"❌ File not found error: {e}\")\n",
        "    print(\"Please ensure the dataset paths in the script are correct.\")\n",
        "    exit() # Exit the script if dataset loading fails\n",
        "\n",
        "# Fake training loop\n",
        "for epoch in range(1,4):\n",
        "    print(f'Epoch {epoch}: training...done!')\n",
        "print('✅ Finished demo training.')\n",
        "\"\"\"\n",
        "with open('/content/drive/MyDrive/retinalnewproject/phase1_demo.py', 'w') as f:\n",
        "    f.write(demo_code)\n",
        "\n",
        "print(\"✅ File created at /content/drive/MyDrive/retinalnewproject/phase1_demo.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRr9JkE1XWOQ",
        "outputId": "ef882f6f-927c-4649-f4dd-8edd3d84aac4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Demo running with Torch version: 2.8.0+cu126\n",
            "Epoch 1: training...done!\n",
            "Epoch 2: training...done!\n",
            "Epoch 3: training...done!\n",
            "✅ Finished demo training.\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/retinalnewproject/phase1_demo.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WobtfNVTYLa-",
        "outputId": "ea172d0c-0a24-4770-fcd3-85e4d101c3aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/retinalnewproject/phase1_demo.py\", line 58, in <module>\n",
            "    train_dataset = RetinalDataset(train_csv, train_dir, transform)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/retinalnewproject/phase1_demo.py\", line 18, in __init__\n",
            "    self.df = pd.read_csv(csv_file)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n",
            "    return _read(filepath_or_buffer, kwds)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 620, in _read\n",
            "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\n",
            "    self._engine = self._make_engine(f, self.engine)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 1880, in _make_engine\n",
            "    self.handles = get_handle(\n",
            "                   ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\", line 873, in get_handle\n",
            "    handle = open(\n",
            "             ^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/retinalnewproject/datasets/ODIR5K/train_1.csv'\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/retinalnewproject/phase1_demo.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XZKjoiZY3gl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/retinalnewproject/datasets/APTOS\"\n",
        "\n",
        "train_csv = os.path.join(base_path, \"train_1.csv\")\n",
        "train_dir = os.path.join(base_path, \"train_images\")\n",
        "\n",
        "val_csv = os.path.join(base_path, \"valid.csv\")\n",
        "val_dir = os.path.join(base_path, \"val_images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvuJHCkOZF0N"
      },
      "outputs": [],
      "source": [
        "base_path = \"/content/drive/MyDrive/retinalnewproject/datasets/ODIR5K\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLw2aXv-ZJFS"
      },
      "outputs": [],
      "source": [
        "base_path = \"/content/drive/MyDrive/retinalnewproject/datasets/RFMiD\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFN26710ZmZn",
        "outputId": "9277f830-3072-41a7-d5aa-83fb69e6a95d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['id_code', 'diagnosis'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_1.csv\")\n",
        "print(df.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbaa2980",
        "outputId": "558c91ad-bdb7-4cc8-f16f-c11182502716"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/retinalnewproject/phase1_demo.py\", line 89, in <module>\n",
            "    for images, labels in train_loader:\n",
            "                          ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 734, in __next__\n",
            "    data = self._next_data()\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 790, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "            ~~~~~~~~~~~~^^^^^\n",
            "  File \"/content/drive/MyDrive/retinalnewproject/phase1_demo.py\", line 47, in __getitem__\n",
            "    image = Image.open(img_path).convert('RGB')\n",
            "            ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/PIL/Image.py\", line 3513, in open\n",
            "    fp = builtins.open(filename, \"rb\")\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images/4e54ccfd49b2.png'\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/retinalnewproject/phase1_demo.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqNj2I15aQ77"
      },
      "outputs": [],
      "source": [
        "# Original\n",
        "train_dir = os.path.join(base_path, \"train_images\")\n",
        "val_dir   = os.path.join(base_path, \"val_images\")\n",
        "\n",
        "# Corrected (include the extra 'test_images' folder)\n",
        "train_dir = os.path.join(base_path, \"train_images\", \"test_images\")\n",
        "val_dir   = os.path.join(base_path, \"val_images\", \"val_images\")  # check val_images folder similarly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sXngSUnaThP",
        "outputId": "1f1dffd9-8c3f-4b76-e7e5-6a5764f954bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_images\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlSPqiDxbfq7",
        "outputId": "dd2be5cc-a20a-4075-d903-096ff636a9ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train folder: /content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images/train_images\n",
            "Val folder:   /content/drive/MyDrive/retinalnewproject/datasets/APTOS/val_images/val_images\n",
            "Test folder:  /content/drive/MyDrive/retinalnewproject/datasets/APTOS/test_images/test_images\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/retinalnewproject/phase1_demo.py\", line 118, in <module>\n",
            "    for images, labels in train_loader:\n",
            "                          ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 734, in __next__\n",
            "    data = self._next_data()\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 790, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "            ~~~~~~~~~~~~^^^^^\n",
            "  File \"/content/drive/MyDrive/retinalnewproject/phase1_demo.py\", line 74, in __getitem__\n",
            "    raise FileNotFoundError(f\"Image file not found for id {row['id_code']} in {self.img_dir}\")\n",
            "FileNotFoundError: Image file not found for id 1df0431bfa73 in /content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images/train_images\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/retinalnewproject/phase1_demo.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 931
        },
        "collapsed": true,
        "id": "0FKYA6ltewtM",
        "outputId": "85c9d27d-eccb-4848-a5fe-68dd714c9e26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train folder: /content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images/train_images\n",
            "Val folder:   /content/drive/MyDrive/retinalnewproject/datasets/APTOS/val_images/val_images\n",
            "Test folder:  /content/drive/MyDrive/retinalnewproject/datasets/APTOS/test_images/test_images\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "Image file not found for id 916ec976ff30 in /content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images/train_images",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2734753324.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2734753324.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mid_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id_code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mid_code\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_to_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Image file not found for id {id_code} in {self.img_dir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_to_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_code\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Image file not found for id 916ec976ff30 in /content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images/train_images"
          ]
        }
      ],
      "source": [
        "# phase1_demo.py\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# -------------------------------\n",
        "# 0. Dataset Paths\n",
        "# -------------------------------\n",
        "DATASET_NAME = \"APTOS\"\n",
        "base_path = f\"/content/drive/MyDrive/retinalnewproject/datasets/{DATASET_NAME}\"\n",
        "\n",
        "train_csv = os.path.join(base_path, \"train_1.csv\")\n",
        "val_csv   = os.path.join(base_path, \"valid.csv\")\n",
        "test_csv  = os.path.join(base_path, \"test.csv\")  # optional\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Recursive folder detection\n",
        "# -------------------------------\n",
        "def get_image_folder(base_folder):\n",
        "    \"\"\"Recursively find first folder containing image files.\"\"\"\n",
        "    for root, dirs, files in os.walk(base_folder):\n",
        "        img_files = [f for f in files if f.lower().endswith(('.png','.jpg','.jpeg'))]\n",
        "        if len(img_files) > 0:\n",
        "            return root\n",
        "    raise ValueError(f\"No image files found in {base_folder}\")\n",
        "\n",
        "train_dir = get_image_folder(os.path.join(base_path, \"train_images\"))\n",
        "val_dir   = get_image_folder(os.path.join(base_path, \"val_images\"))\n",
        "test_dir  = get_image_folder(os.path.join(base_path, \"test_images\"))\n",
        "\n",
        "print(f\"Train folder: {train_dir}\")\n",
        "print(f\"Val folder:   {val_dir}\")\n",
        "print(f\"Test folder:  {test_dir}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Dataset Class\n",
        "# -------------------------------\n",
        "class RetinalDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, label_col='diagnosis', transform=None):\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.label_col = label_col\n",
        "\n",
        "        self.df['id_code'] = self.df['id_code'].astype(str).str.strip()\n",
        "\n",
        "        # Map labels to integers if not numeric\n",
        "        if not np.issubdtype(self.df[self.label_col].dtype, np.integer):\n",
        "            self.df[self.label_col] = pd.Categorical(self.df[self.label_col]).codes\n",
        "\n",
        "        # Build mapping from id_code -> full image path\n",
        "        self.id_to_path = {}\n",
        "        for root, dirs, files in os.walk(self.img_dir):\n",
        "            for f in files:\n",
        "                name, ext = os.path.splitext(f)\n",
        "                if name in self.df['id_code'].values and ext.lower() in ['.png','.jpg','.jpeg']:\n",
        "                    self.id_to_path[name] = os.path.join(root, f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        id_code = row['id_code']\n",
        "        if id_code not in self.id_to_path:\n",
        "            raise FileNotFoundError(f\"Image file not found for id {id_code} in {self.img_dir}\")\n",
        "        img_path = self.id_to_path[id_code]\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = torch.tensor(row[self.label_col], dtype=torch.long)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Transformations\n",
        "# -------------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Load datasets\n",
        "# -------------------------------\n",
        "train_dataset = RetinalDataset(train_csv, train_dir, label_col='diagnosis', transform=transform)\n",
        "val_dataset   = RetinalDataset(val_csv, val_dir, label_col='diagnosis', transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Model\n",
        "# -------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = len(pd.unique(train_dataset.df['diagnosis']))\n",
        "model = models.efficientnet_b0(weights=None, num_classes=num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Training loop\n",
        "# -------------------------------\n",
        "epochs = 3  # increase for better accuracy\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 7. Validation\n",
        "# -------------------------------\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "f1  = f1_score(all_labels, all_preds, average='weighted')\n",
        "print(f\"Validation Accuracy: {acc:.4f} | F1 Score: {f1:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 8. Save model\n",
        "# -------------------------------\n",
        "results_path = \"/content/drive/MyDrive/retinalnewproject/results\"\n",
        "os.makedirs(results_path, exist_ok=True)\n",
        "torch.save(model.state_dict(), os.path.join(results_path, f\"{DATASET_NAME}_efficientnet_phase1.pth\"))\n",
        "print(f\"✅ Model saved to {results_path}/{DATASET_NAME}_efficientnet_phase1.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPweMqesfRkU",
        "outputId": "9f44b05d-ea98-4627-8009-943dfe6d731e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Train folder: /content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images/train_images\n",
            "Val folder:   /content/drive/MyDrive/retinalnewproject/datasets/APTOS/val_images/val_images\n",
            "Test folder:  /content/drive/MyDrive/retinalnewproject/datasets/APTOS/test_images/test_images\n",
            "Using 967 images from /content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images/train_images\n",
            "Using 112 images from /content/drive/MyDrive/retinalnewproject/datasets/APTOS/val_images/val_images\n",
            "Epoch 1/3 | Train Loss: 1.3486\n",
            "Epoch 2/3 | Train Loss: 0.7365\n",
            "Epoch 3/3 | Train Loss: 0.6886\n",
            "Validation Accuracy: 0.9286 | F1 Score: 0.9157\n",
            "✅ Model saved to /content/drive/MyDrive/retinalnewproject/results/APTOS_efficientnet_phase1.pth\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!python /content/drive/MyDrive/retinalnewproject/phase1_demo.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "qNnpkVYJkFLj",
        "outputId": "ad6305aa-64df-4d6f-a2d3-87f89a37b98a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f6c32cdd5511314805.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://f6c32cdd5511314805.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Install Gradio if not installed\n",
        "!pip install gradio --quiet\n",
        "\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# -------------------\n",
        "# Settings\n",
        "# -------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "num_classes = 5  # change to your number of disease classes\n",
        "model_path = \"/content/drive/MyDrive/retinalnewproject/results/APTOS_efficientnet_phase1.pth\"\n",
        "\n",
        "# -------------------\n",
        "# Image preprocessing\n",
        "# -------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# -------------------\n",
        "# Load model\n",
        "# -------------------\n",
        "model = models.efficientnet_b0(weights=None, num_classes=num_classes)\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# -------------------\n",
        "# Grad-CAM function\n",
        "# -------------------\n",
        "def grad_cam(input_image, class_idx=None):\n",
        "    # Hook for gradients\n",
        "    gradients = []\n",
        "    def save_gradients(module, grad_input, grad_output):\n",
        "        gradients.append(grad_output[0])\n",
        "    target_layer = model.features[-1]\n",
        "    target_layer.register_backward_hook(save_gradients)\n",
        "\n",
        "    # Prepare image\n",
        "    input_tensor = transform(input_image).unsqueeze(0).to(device)\n",
        "    input_tensor.requires_grad = True\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(input_tensor)\n",
        "    if class_idx is None:\n",
        "        class_idx = torch.argmax(output, dim=1).item()\n",
        "\n",
        "    # Backward pass\n",
        "    model.zero_grad()\n",
        "    loss = output[0, class_idx]\n",
        "    loss.backward()\n",
        "\n",
        "    grads = gradients[0].cpu().data.numpy()[0]\n",
        "    fmap = target_layer(input_tensor).cpu().data.numpy()[0]\n",
        "\n",
        "    # Grad-CAM calculation\n",
        "    weights = np.mean(grads, axis=(1,2))\n",
        "    cam = np.zeros(fmap.shape[1:], dtype=np.float32)\n",
        "    for i, w in enumerate(weights):\n",
        "        cam += w * fmap[i]\n",
        "    cam = np.maximum(cam, 0)\n",
        "    cam = cam / cam.max()\n",
        "    cam = np.uint8(cam * 255)\n",
        "    cam = Image.fromarray(cam).resize(input_image.size, Image.ANTIALIAS)\n",
        "    return cam\n",
        "\n",
        "# -------------------\n",
        "# Prediction + Grad-CAM\n",
        "# -------------------\n",
        "def predict_with_gradcam(img):\n",
        "    # Predict\n",
        "    input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "    output = model(input_tensor)\n",
        "    pred_idx = torch.argmax(output, dim=1).item()\n",
        "\n",
        "    # Map class index to label (adjust this list to your dataset)\n",
        "    labels = ['Normal', 'Diabetic Retinopathy', 'Glaucoma', 'Cataract', 'Other']\n",
        "    pred_label = labels[pred_idx]\n",
        "\n",
        "    # Grad-CAM overlay\n",
        "    cam = grad_cam(img, class_idx=pred_idx)\n",
        "    img_with_cam = Image.blend(img.convert(\"RGB\"), cam.convert(\"RGB\"), alpha=0.5)\n",
        "\n",
        "    return pred_label, img_with_cam\n",
        "\n",
        "# -------------------\n",
        "# Launch Gradio app\n",
        "# -------------------\n",
        "interface = gr.Interface(\n",
        "    fn=predict_with_gradcam,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=[gr.Textbox(label=\"Predicted Disease\"), gr.Image(label=\"Grad-CAM Heatmap\")],\n",
        "    title=\"Retinal Disease Detection\",\n",
        "    description=\"Upload a fundus image to get predicted disease and Grad-CAM heatmap.\"\n",
        ")\n",
        "\n",
        "interface.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "t8NKpU8Qk5Hu",
        "outputId": "9c6a54df-227e-4f12-c4ec-5199a0cb45a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5799619c6f7b159e42.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://5799619c6f7b159e42.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install --upgrade gradio --quiet\n",
        "\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# -------------------\n",
        "# Settings\n",
        "# -------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "num_classes = 5\n",
        "model_path = \"/content/drive/MyDrive/retinalnewproject/results/APTOS_efficientnet_phase1.pth\"\n",
        "\n",
        "# -------------------\n",
        "# Preprocessing\n",
        "# -------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# -------------------\n",
        "# Load model\n",
        "# -------------------\n",
        "model = models.efficientnet_b0(weights=None, num_classes=num_classes)\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# -------------------\n",
        "# Grad-CAM function\n",
        "# -------------------\n",
        "def grad_cam(input_image, class_idx=None):\n",
        "    gradients = []\n",
        "\n",
        "    def save_gradients(module, grad_input, grad_output):\n",
        "        gradients.append(grad_output[0])\n",
        "\n",
        "    target_layer = model.features[-1]\n",
        "    target_layer.register_full_backward_hook(save_gradients)\n",
        "\n",
        "    input_tensor = transform(input_image).unsqueeze(0).to(device)\n",
        "    input_tensor.requires_grad = True\n",
        "    output = model(input_tensor)\n",
        "    if class_idx is None:\n",
        "        class_idx = torch.argmax(output, dim=1).item()\n",
        "    model.zero_grad()\n",
        "    loss = output[0, class_idx]\n",
        "    loss.backward()\n",
        "\n",
        "    grads = gradients[0].cpu().data.numpy()[0]\n",
        "    fmap = target_layer(input_tensor).cpu().data.numpy()[0]\n",
        "    weights = np.mean(grads, axis=(1, 2))\n",
        "    cam = np.zeros(fmap.shape[1:], dtype=np.float32)\n",
        "    for i, w in enumerate(weights):\n",
        "        cam += w * fmap[i]\n",
        "    cam = np.maximum(cam, 0)\n",
        "    cam = cam / (cam.max() + 1e-8)\n",
        "    cam = np.uint8(cam * 255)\n",
        "    cam = Image.fromarray(cam).resize(input_image.size, Image.ANTIALIAS).convert(\"RGB\")\n",
        "    return cam\n",
        "\n",
        "# -------------------\n",
        "# Prediction + Grad-CAM\n",
        "# -------------------\n",
        "labels = ['Normal', 'Diabetic Retinopathy', 'Glaucoma', 'Cataract', 'Other']\n",
        "\n",
        "def predict_multiple(file_paths):\n",
        "    preds = []\n",
        "    cams = []\n",
        "    if not isinstance(file_paths, list):\n",
        "        file_paths = [file_paths]\n",
        "\n",
        "    for f in file_paths:\n",
        "        img = Image.open(f).convert(\"RGB\")\n",
        "        input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "        output = model(input_tensor)\n",
        "        pred_idx = torch.argmax(output, dim=1).item()\n",
        "        pred_label = labels[pred_idx]\n",
        "        preds.append(f\"{os.path.basename(f)}: {pred_label}\")\n",
        "\n",
        "        cam = grad_cam(img, class_idx=pred_idx)\n",
        "        blended = Image.blend(img, cam, alpha=0.5)\n",
        "        cams.append(blended)\n",
        "\n",
        "    # Return predictions as a single string\n",
        "    return \"\\n\".join(preds), cams\n",
        "\n",
        "# -------------------\n",
        "# Gradio Interface\n",
        "# -------------------\n",
        "interface = gr.Interface(\n",
        "    fn=predict_multiple,\n",
        "    inputs=gr.File(file_types=[\".png\", \".jpg\", \".jpeg\"], file_count=\"multiple\", label=\"Upload Fundus Images\"),\n",
        "    outputs=[gr.Textbox(label=\"Predicted Diseases\"), gr.Gallery(label=\"Grad-CAM Heatmaps\")],\n",
        "    title=\"Retinal Disease Detection\",\n",
        "    description=\"Upload one or more fundus images to get predictions and Grad-CAM heatmaps.\"\n",
        ")\n",
        "\n",
        "interface.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NqOuYjJb_RJ",
        "outputId": "a7438321-c211-4fa2-a99c-359137f52368"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset: DRIVE\n",
            "  CSV files:\n",
            "  Image folders:\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/DRIVE/DRIVE (2 files)\n",
            "      Sample images: ['training', 'test']\n",
            "\n",
            "Dataset: .ipynb_checkpoints\n",
            "  CSV files:\n",
            "  Image folders:\n",
            "\n",
            "Dataset: APTOS\n",
            "  CSV files:\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/APTOS/test.csv\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_1.csv\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/APTOS/valid.csv\n",
            "  Image folders:\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/APTOS/val_images (1 files)\n",
            "      Sample images: ['val_images']\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images (1 files)\n",
            "      Sample images: ['train_images']\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/APTOS/test_images (1 files)\n",
            "      Sample images: ['test_images']\n",
            "\n",
            "Dataset: CHASEDB1\n",
            "  CSV files:\n",
            "  Image folders:\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/CHASEDB1/Images (28 files)\n",
            "      Sample images: ['Image_12L.jpg', 'Image_14R.jpg', 'Image_06R.jpg', 'Image_04R.jpg', 'Image_10L.jpg']\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/CHASEDB1/Masks (28 files)\n",
            "      Sample images: ['Image_03L_1stHO.png', 'Image_12R_1stHO.png', 'Image_05R_1stHO.png', 'Image_13R_1stHO.png', 'Image_14L_1stHO.png']\n",
            "\n",
            "Dataset: Messidor2\n",
            "  CSV files:\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/Messidor2/messidor_data.csv\n",
            "  Image folders:\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/Messidor2/messidor-2 (1 files)\n",
            "      Sample images: ['messidor-2']\n",
            "\n",
            "Dataset: ODIR5K\n",
            "  CSV files:\n",
            "  Image folders:\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/ODIR5K/datasets (8 files)\n",
            "      Sample images: ['normal', 'ageDegeneration', 'hypertension', 'myopia', 'others']\n",
            "\n",
            "Dataset: RFMiD\n",
            "  CSV files:\n",
            "  Image folders:\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/RFMiD/Test_Set (1 files)\n",
            "      Sample images: ['Test_Set']\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/RFMiD/Evaluation_Set (1 files)\n",
            "      Sample images: ['Evaluation_Set']\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/RFMiD/Training_Set (1 files)\n",
            "      Sample images: ['Training_Set']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Base dataset folder\n",
        "dataset_base = \"/content/drive/MyDrive/retinalnewproject/datasets\"\n",
        "\n",
        "# List all datasets\n",
        "for dataset_name in os.listdir(dataset_base):\n",
        "    dataset_path = os.path.join(dataset_base, dataset_name)\n",
        "    if os.path.isdir(dataset_path):\n",
        "        print(f\"\\nDataset: {dataset_name}\")\n",
        "\n",
        "        # List CSV files\n",
        "        csv_files = [f for f in os.listdir(dataset_path) if f.endswith(\".csv\")]\n",
        "        print(\"  CSV files:\")\n",
        "        for f in csv_files:\n",
        "            print(f\"    {os.path.join(dataset_path, f)}\")\n",
        "\n",
        "        # List image folders and first few images\n",
        "        img_folders = [f for f in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, f))]\n",
        "        print(\"  Image folders:\")\n",
        "        for f in img_folders:\n",
        "            img_folder_path = os.path.join(dataset_path, f)\n",
        "            imgs = os.listdir(img_folder_path)\n",
        "            print(f\"    {img_folder_path} ({len(imgs)} files)\")\n",
        "            print(f\"      Sample images: {imgs[:5]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QWBWlzgcoSR"
      },
      "outputs": [],
      "source": [
        "def get_actual_image_folder(base_folder):\n",
        "    subfolders = [f for f in os.listdir(base_folder) if os.path.isdir(os.path.join(base_folder, f))]\n",
        "    if len(subfolders) == 0:\n",
        "        raise ValueError(f\"No subfolders found in {base_folder}\")\n",
        "    return os.path.join(base_folder, subfolders[0])\n",
        "\n",
        "train_dir = get_actual_image_folder(\"/content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images\")\n",
        "val_dir   = get_actual_image_folder(\"/content/drive/MyDrive/retinalnewproject/datasets/APTOS/val_images\")\n",
        "test_dir  = get_actual_image_folder(\"/content/drive/MyDrive/retinalnewproject/datasets/APTOS/test_images\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "CT0uNRdLQ1V0",
        "outputId": "3a07bccc-281f-4bc4-97d2-312aeb12c202"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV: /content/drive/MyDrive/retinalnewproject/datasets/Messidor2/messidor_data.csv\n",
            "Images: /content/drive/MyDrive/retinalnewproject/datasets/Messidor2/messidor-2\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "Image for ID 20051020_43808_0100_PP.png not found in /content/drive/MyDrive/retinalnewproject/datasets/Messidor2/messidor-2",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4270180787.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# -------- Data Split --------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mfull_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMessidorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mval_size\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_ds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4270180787.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, csv_path, img_dir, transform)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Image for ID {i} not found in {img_dir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Image for ID 20051020_43808_0100_PP.png not found in /content/drive/MyDrive/retinalnewproject/datasets/Messidor2/messidor-2"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "# Messidor2 + ResNet50 Training\n",
        "# ===============================\n",
        "import os, glob\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms, models\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# -------- Paths --------\n",
        "base_path   = \"/content/drive/MyDrive/retinalnewproject/datasets/Messidor2\"\n",
        "csv_file    = os.path.join(base_path, \"messidor_data.csv\")   # <-- ensure columns: id_code, diagnosis\n",
        "img_dir     = os.path.join(base_path, \"messidor-2\")\n",
        "results_dir = \"/content/drive/MyDrive/retinalnewproject/results\"\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "print(\"CSV:\", csv_file)\n",
        "print(\"Images:\", img_dir)\n",
        "\n",
        "# -------- Dataset --------\n",
        "class MessidorDataset(Dataset):\n",
        "    def __init__(self, csv_path, img_dir, transform=None):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        # adapt these if different\n",
        "        self.img_ids = self.df[\"id_code\"].tolist()\n",
        "        self.labels  = self.df[\"diagnosis\"].tolist()\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # map available files for robust loading\n",
        "        all_imgs = {os.path.splitext(os.path.basename(p))[0]: p\n",
        "                    for p in glob.glob(os.path.join(img_dir, \"*\"))}\n",
        "\n",
        "        self.paths = []\n",
        "        for i in self.img_ids:\n",
        "            p = all_imgs.get(str(i))\n",
        "            if p is None:\n",
        "                raise FileNotFoundError(f\"Image for ID {i} not found in {img_dir}\")\n",
        "            self.paths.append(p)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        label = int(self.labels[idx])\n",
        "        if self.transform: image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# -------- Transforms --------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# -------- Data Split --------\n",
        "full_ds = MessidorDataset(csv_file, img_dir, transform)\n",
        "train_size = int(0.8 * len(full_ds))\n",
        "val_size   = len(full_ds) - train_size\n",
        "train_ds, val_ds = random_split(full_ds, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Train size: {len(train_ds)}, Val size: {len(val_ds)}\")\n",
        "\n",
        "# -------- Model --------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = len(set(full_ds.labels))\n",
        "\n",
        "model = models.resnet50(weights=\"IMAGENET1K_V1\")  # pretrained\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False  # freeze backbone\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)  # new head\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3)\n",
        "\n",
        "# -------- Training --------\n",
        "epochs = 3\n",
        "for epoch in range(1, epochs+1):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()*imgs.size(0)\n",
        "    print(f\"Epoch {epoch}/{epochs} | Train Loss: {running_loss/len(train_loader.dataset):.4f}\")\n",
        "\n",
        "# -------- Validation --------\n",
        "model.eval()\n",
        "preds, gts = [], []\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        outputs = model(imgs)\n",
        "        preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
        "        gts.extend(labels.numpy())\n",
        "\n",
        "acc = accuracy_score(gts, preds)\n",
        "f1  = f1_score(gts, preds, average=\"weighted\")\n",
        "print(f\"Validation Accuracy: {acc:.4f} | F1 Score: {f1:.4f}\")\n",
        "\n",
        "# -------- Save --------\n",
        "save_path = os.path.join(results_dir, \"Messidor2_resnet50_phase1.pth\")\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"✅ Model saved to {save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXehQgWURtft",
        "outputId": "d10c2f25-830f-4ab3-8335-3a2d2ed564a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total images found: 1\n",
            "Sample names: ['messidor-2']\n"
          ]
        }
      ],
      "source": [
        "import os, glob\n",
        "\n",
        "img_dir = \"/content/drive/MyDrive/retinalnewproject/datasets/Messidor2/messidor-2\"\n",
        "files = [os.path.basename(p) for p in glob.glob(os.path.join(img_dir, \"*\"))]\n",
        "\n",
        "print(\"Total images found:\", len(files))\n",
        "print(\"Sample names:\", files[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTyYE2N6SV2M",
        "outputId": "25b15fb2-eeac-44b1-a7d6-440d32749730"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contents: ['messidor-2']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "root = \"/content/drive/MyDrive/retinalnewproject/datasets/Messidor2/messidor-2\"\n",
        "print(\"Contents:\", os.listdir(root))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfdnCmv4Sldg",
        "outputId": "295b3dd1-c8e5-4611-b857-07d827f29d0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contents: ['preprocess']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "deep_path = \"/content/drive/MyDrive/retinalnewproject/datasets/Messidor2/messidor-2/messidor-2\"\n",
        "print(\"Contents:\", os.listdir(deep_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIJyudQvSwwQ",
        "outputId": "bd5d35ea-38ba-4321-aa39-c61425b7e286"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contents: ['20060412_51775_0200_PP.png', '20060411_61979_0200_PP.png', '20060411_62142_0200_PP.png', '20060411_59549_0200_PP.png', '20060411_59812_0200_PP.png', '20060412_52425_0200_PP.png', '20060412_51746_0200_PP.png', '20060412_52371_0200_PP.png', '20060411_61060_0200_PP.png', '20060411_59747_0200_PP.png', '20060411_61402_0200_PP.png', '20060411_58413_0200_PP.png', '20060411_58494_0200_PP.png', '20060411_58993_0200_PP.png', '20060411_62036_0200_PP.png', '20060411_59176_0200_PP.png', '20060412_52020_0200_PP.png', '20060411_60426_0200_PP.png', '20060411_60028_0200_PP.png', '20060412_52351_0200_PP.png']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "deepest_path = \"/content/drive/MyDrive/retinalnewproject/datasets/Messidor2/messidor-2/messidor-2/preprocess\"\n",
        "print(\"Contents:\", os.listdir(deepest_path)[:20])  # show first 20 entries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "JH1GZCPaTMUr",
        "outputId": "7f7ed80e-15ad-4406-fc3e-192c6e8ece16"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'image'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'image'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3733735946.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# Data Split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# -------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mfull_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMessidorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mval_size\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_ds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3733735946.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, csv_path, img_dir, transform)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mimg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;31m# Convert generator to list before going through hashable part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'image'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# -------------------------\n",
        "# Paths  ✅  (UPDATED)\n",
        "# -------------------------\n",
        "csv_file = \"/content/drive/MyDrive/retinalnewproject/datasets/Messidor2/messidor_data.csv\"\n",
        "img_dir  = \"/content/drive/MyDrive/retinalnewproject/datasets/Messidor2/messidor-2/messidor-2/preprocess\"\n",
        "\n",
        "# -------------------------\n",
        "# Dataset\n",
        "# -------------------------\n",
        "class MessidorDataset(Dataset):\n",
        "    def __init__(self, csv_path, img_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Build a map for quick lookup\n",
        "        all_imgs = {os.path.basename(x): os.path.join(root, x)\n",
        "                    for root, _, files in os.walk(img_dir)\n",
        "                    for x in files}\n",
        "\n",
        "        self.paths, self.labels = [], []\n",
        "        for _, row in self.data.iterrows():\n",
        "            img_name = row['image']\n",
        "            label = row['label']\n",
        "            p = all_imgs.get(img_name)\n",
        "            if p is None:\n",
        "                raise FileNotFoundError(f\"Image for ID {img_name} not found in {img_dir}\")\n",
        "            self.paths.append(p)\n",
        "            self.labels.append(int(label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, self.labels[idx]\n",
        "\n",
        "# -------------------------\n",
        "# Transforms\n",
        "# -------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# -------------------------\n",
        "# Data Split\n",
        "# -------------------------\n",
        "full_ds = MessidorDataset(csv_file, img_dir, transform)\n",
        "train_size = int(0.8 * len(full_ds))\n",
        "val_size   = len(full_ds) - train_size\n",
        "train_ds, val_ds = random_split(full_ds, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"✅ Total images: {len(full_ds)} | Train: {train_size} | Val: {val_size}\")\n",
        "\n",
        "# -------------------------\n",
        "# Model\n",
        "# -------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.efficientnet_b0(pretrained=True)\n",
        "model.classifier[1] = nn.Linear(model.classifier[1].in_features, 2)  # Assuming 2 classes\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# -------------------------\n",
        "# Training Loop\n",
        "# -------------------------\n",
        "for epoch in range(3):  # change epochs if needed\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch [{epoch+1}/3] | Train Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# -------------------------\n",
        "# Validation\n",
        "# -------------------------\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "f1  = f1_score(all_labels, all_preds, average='macro')\n",
        "print(f\"✅ Validation Accuracy: {acc:.4f} | F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Save model\n",
        "os.makedirs(\"/content/drive/MyDrive/retinalnewproject/results\", exist_ok=True)\n",
        "torch.save(model.state_dict(),\n",
        "           \"/content/drive/MyDrive/retinalnewproject/results/Messidor2_efficientnet_phase1.pth\")\n",
        "print(\"💾 Model saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmGy1ySrTb0f",
        "outputId": "6221d756-891e-4f6e-9180-a30bc2deb1f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in CSV: ['id_code', 'diagnosis', 'adjudicated_dme', 'adjudicated_gradable']\n",
            "                      id_code  diagnosis  adjudicated_dme  \\\n",
            "0  20051020_43808_0100_PP.png          0                0   \n",
            "1  20051020_43832_0100_PP.png          1                0   \n",
            "2  20051020_43882_0100_PP.png          1                0   \n",
            "3  20051020_43906_0100_PP.png          2                1   \n",
            "4  20051020_44261_0100_PP.png          0                0   \n",
            "\n",
            "   adjudicated_gradable  \n",
            "0                     1  \n",
            "1                     1  \n",
            "2                     1  \n",
            "3                     1  \n",
            "4                     1  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "csv_file = \"/content/drive/MyDrive/retinalnewproject/datasets/Messidor2/messidor_data.csv\"\n",
        "df = pd.read_csv(csv_file)\n",
        "print(\"Columns in CSV:\", df.columns.tolist())\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8uotAK_TwmA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class MessidorDataset(Dataset):\n",
        "    def __init__(self, csv_path, img_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Build a mapping of all images present in the directory for fast lookup\n",
        "        all_imgs = {f: os.path.join(root, f)\n",
        "                    for root, _, files in os.walk(img_dir)\n",
        "                    for f in files}\n",
        "\n",
        "        self.paths, self.labels = [], []\n",
        "        for _, row in self.data.iterrows():\n",
        "            img_name = row['id_code']       # already includes .png\n",
        "            label    = row['diagnosis']     # use the diagnosis column\n",
        "            p = all_imgs.get(img_name)\n",
        "            if p is None:\n",
        "                raise FileNotFoundError(f\"Image {img_name} not found in {img_dir}\")\n",
        "            self.paths.append(p)\n",
        "            self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, torch.tensor(self.labels[idx], dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYDVWo5kT_Yu"
      },
      "outputs": [],
      "source": [
        "img_dir = \"/content/drive/MyDrive/retinalnewproject/datasets/Messidor2/messidor-2/preprocess\"\n",
        "csv_file = \"/content/drive/MyDrive/retinalnewproject/datasets/Messidor2/messidor_data.csv\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHk6D8o4Ure3",
        "outputId": "6f3e0335-8a96-4a93-a9a0-7c84dcfce6e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using image root: /content/drive/MyDrive/retinalnewproject/datasets/Messidor2/messidor-2/messidor-2/preprocess\n",
            "CSV columns: ['id_code', 'diagnosis', 'adjudicated_dme', 'adjudicated_gradable']\n",
            "                      id_code  diagnosis  adjudicated_dme  \\\n",
            "0  20051020_43808_0100_PP.png          0                0   \n",
            "1  20051020_43832_0100_PP.png          1                0   \n",
            "2  20051020_43882_0100_PP.png          1                0   \n",
            "3  20051020_43906_0100_PP.png          2                1   \n",
            "4  20051020_44261_0100_PP.png          0                0   \n",
            "\n",
            "   adjudicated_gradable  \n",
            "0                     1  \n",
            "1                     1  \n",
            "2                     1  \n",
            "3                     1  \n",
            "4                     1  \n",
            "Dataset: found 1744 images (skipped 0 missing)\n",
            "Train: 1395, Val: 349\n",
            "Sanity batch shapes: torch.Size([16, 3, 224, 224]) torch.Size([16])\n",
            "Num classes: 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 149MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3 | train_loss=1.1092 | val_acc=0.5415 | val_f1=0.2442 | time=528.9s\n",
            "Epoch 2/3 | train_loss=1.0351 | val_acc=0.5387 | val_f1=0.2380 | time=509.9s\n"
          ]
        }
      ],
      "source": [
        "# Paste this whole cell into Colab (below the drive.mount cell)\n",
        "import os, glob, time\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms, models\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# ---------- SETTINGS ----------\n",
        "BASE = \"/content/drive/MyDrive/retinalnewproject/datasets/Messidor2\"\n",
        "CSV = os.path.join(BASE, \"messidor_data.csv\")\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/retinalnewproject/results\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# ---------- Helper: find folder that actually contains images ----------\n",
        "def find_image_root(base_folder, min_images=10):\n",
        "    \"\"\"Walk base_folder recursively and return the first folder that has at least min_images image files.\"\"\"\n",
        "    exts = (\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\", \".bmp\")\n",
        "    for root, dirs, files in os.walk(base_folder):\n",
        "        imgs = [f for f in files if f.lower().endswith(exts)]\n",
        "        if len(imgs) >= min_images:\n",
        "            return root\n",
        "    # fallback: return deepest folder with any images\n",
        "    best = None\n",
        "    best_count = 0\n",
        "    for root, dirs, files in os.walk(base_folder):\n",
        "        imgs = [f for f in files if f.lower().endswith(exts)]\n",
        "        if len(imgs) > best_count:\n",
        "            best_count = len(imgs); best = root\n",
        "    return best\n",
        "\n",
        "img_root = find_image_root(BASE, min_images=20)\n",
        "print(\"Using image root:\", img_root)\n",
        "if img_root is None:\n",
        "    raise RuntimeError(f\"No image files found under {BASE} — check your dataset location.\")\n",
        "\n",
        "# ---------- Inspect CSV ----------\n",
        "df = pd.read_csv(CSV)\n",
        "print(\"CSV columns:\", df.columns.tolist())\n",
        "print(df.head())\n",
        "\n",
        "# ---------- Dataset class (robust lookup by basename) ----------\n",
        "class MessidorDataset(Dataset):\n",
        "    def __init__(self, csv_path, img_root, img_col='id_code', label_col='diagnosis', transform=None):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.img_root = img_root\n",
        "        self.img_col = img_col\n",
        "        self.label_col = label_col\n",
        "        self.transform = transform\n",
        "\n",
        "        # Build recursive mapping basename -> full path\n",
        "        self.all_imgs = {}\n",
        "        for p in glob.glob(os.path.join(self.img_root, \"**\", \"*.*\"), recursive=True):\n",
        "            name = os.path.basename(p)\n",
        "            stem = os.path.splitext(name)[0]\n",
        "            # prefer the exact name key (with extension) AND also store stem key for extension-agnostic lookup\n",
        "            self.all_imgs[name] = p\n",
        "            self.all_imgs[stem] = p\n",
        "\n",
        "        # Prepare final lists, and collect missing ids (if any)\n",
        "        self.paths = []\n",
        "        self.labels = []\n",
        "        missing = []\n",
        "        for _, row in self.df.iterrows():\n",
        "            raw_name = str(row[self.img_col]).strip()\n",
        "            # try exact match first (filename may include extension), then stem match\n",
        "            if raw_name in self.all_imgs:\n",
        "                p = self.all_imgs[raw_name]\n",
        "            else:\n",
        "                stem = os.path.splitext(raw_name)[0]\n",
        "                p = self.all_imgs.get(stem)\n",
        "            if p is None:\n",
        "                missing.append(raw_name)\n",
        "            else:\n",
        "                self.paths.append(p)\n",
        "                self.labels.append(int(row[self.label_col]))\n",
        "        if len(missing) > 0:\n",
        "            print(f\"⚠️ Warning: {len(missing)} CSV image ids not found in {img_root}.\")\n",
        "            print(\" Sample missing ids:\", missing[:10])\n",
        "        print(f\"Dataset: found {len(self.paths)} images (skipped {len(missing)} missing)\")\n",
        "        if len(self.paths) == 0:\n",
        "            raise RuntimeError(\"No images matched — check CSV 'id_code' values and image folder.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return img, label\n",
        "\n",
        "# ---------- Transforms ----------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "# ---------- Build datasets / loaders ----------\n",
        "full_ds = MessidorDataset(CSV, img_root, img_col='id_code', label_col='diagnosis', transform=transform)\n",
        "train_size = int(0.8 * len(full_ds))\n",
        "val_size = len(full_ds) - train_size\n",
        "train_ds, val_ds = random_split(full_ds, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}\")\n",
        "\n",
        "# ---------- Sanity check: one batch ----------\n",
        "try:\n",
        "    batch_imgs, batch_labels = next(iter(train_loader))\n",
        "    print(\"Sanity batch shapes:\", batch_imgs.shape, batch_labels.shape)\n",
        "except Exception as e:\n",
        "    print(\"Sanity check failed:\", e)\n",
        "\n",
        "# ---------- Model: ResNet-50 (transfer learning) ----------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = len(set(full_ds.labels))\n",
        "print(\"Num classes:\", num_classes)\n",
        "\n",
        "model = models.resnet50(pretrained=True)\n",
        "# freeze backbone (optional) — comment these two lines to fine-tune everything\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "# replace the classifier\n",
        "in_features = model.fc.in_features\n",
        "model.fc = nn.Linear(in_features, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3)\n",
        "\n",
        "# ---------- Training loop (small run) ----------\n",
        "EPOCHS = 3\n",
        "best_val_acc = 0.0\n",
        "metrics_rows = []\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    t0 = time.time()\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs = imgs.to(device); labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * imgs.size(0)\n",
        "    train_loss = total_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    all_preds, all_gts = [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            outputs = model(imgs)\n",
        "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_gts.extend(labels.numpy())\n",
        "    val_acc = accuracy_score(all_gts, all_preds)\n",
        "    val_f1  = f1_score(all_gts, all_preds, average='macro')\n",
        "    metrics_rows.append([epoch, train_loss, val_acc, val_f1])\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_path = os.path.join(RESULTS_DIR, \"Messidor2_resnet50_best.pth\")\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | train_loss={train_loss:.4f} | val_acc={val_acc:.4f} | val_f1={val_f1:.4f} | time={time.time()-t0:.1f}s\")\n",
        "\n",
        "# Save final model & metrics CSV\n",
        "final_path = os.path.join(RESULTS_DIR, \"Messidor2_resnet50_final.pth\")\n",
        "torch.save(model.state_dict(), final_path)\n",
        "metrics_df = pd.DataFrame(metrics_rows, columns=[\"epoch\",\"train_loss\",\"val_acc\",\"val_f1\"])\n",
        "metrics_df.to_csv(os.path.join(RESULTS_DIR, \"Messidor2_metrics.csv\"), index=False)\n",
        "print(\"Saved final model:\", final_path)\n",
        "print(\"Saved metrics:\", os.path.join(RESULTS_DIR, \"Messidor2_metrics.csv\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WE5KfsehdiNG"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "jA1qn8fMduA5",
        "outputId": "7da2f351-3c30-4021-e6f1-00e38a5c5b0d"
      },
      "outputs": [
        {
          "ename": "IndentationError",
          "evalue": "unexpected indent (ipython-input-3146123459.py, line 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3146123459.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    Messidor2_resnet50_best.pth    ← best model (from val accuracy)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ],
      "source": [
        "/content/drive/MyDrive/retinalnewproject/results/\n",
        "    Messidor2_resnet50_best.pth    ← best model (from val accuracy)\n",
        "    Messidor2_resnet50_final.pth   ← final model\n",
        "    Messidor2_metrics.csv          ← training metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "r27aTAiGeEwt",
        "outputId": "99547268-512b-4242-99d2-19a8640c0bb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 | Train Loss: 0.9973 | Val Acc: 0.6619\n",
            "✅ Saved new best model\n",
            "Epoch 2/10 | Train Loss: 0.8040 | Val Acc: 0.6734\n",
            "✅ Saved new best model\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1804206208.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# === Fine-tune Messidor2 with ResNet50 ===\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models\n",
        "\n",
        "# Load the same dataset objects you already defined:\n",
        "# train_loader, val_loader, MessidorDataset etc.\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load pretrained ResNet50\n",
        "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Replace final layer (Messidor2 has 5 classes)\n",
        "model.fc = nn.Linear(model.fc.in_features, 5)\n",
        "\n",
        "# ---- UNFREEZE the backbone ----\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Adam optimizer with smaller LR for fine-tuning\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "EPOCHS = 10\n",
        "best_acc = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # Training\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    val_acc = correct / total\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
        "          f\"Train Loss: {running_loss/len(train_loader.dataset):.4f} | \"\n",
        "          f\"Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(),\n",
        "                   \"/content/drive/MyDrive/retinalnewproject/results/Messidor2_resnet50_finetuned_best.pth\")\n",
        "        print(\"✅ Saved new best model\")\n",
        "\n",
        "print(\"Best validation accuracy:\", best_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRLcfVERmq7w",
        "outputId": "4168153b-3132-4505-c760-d6dd6772daa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[01;34m/content/drive/MyDrive/retinalnewproject\u001b[0m\n",
            "├── \u001b[01;34mdatasets\u001b[0m\n",
            "│   ├── \u001b[01;34mAPTOS\u001b[0m\n",
            "│   │   ├── \u001b[01;34mtest_images\u001b[0m\n",
            "│   │   ├── \u001b[01;34mtrain_images\u001b[0m\n",
            "│   │   └── \u001b[01;34mval_images\u001b[0m\n",
            "│   ├── \u001b[01;34mCHASEDB1\u001b[0m\n",
            "│   │   ├── \u001b[01;34mImages\u001b[0m\n",
            "│   │   └── \u001b[01;34mMasks\u001b[0m\n",
            "│   ├── \u001b[01;34mDRIVE\u001b[0m\n",
            "│   │   └── \u001b[01;34mDRIVE\u001b[0m\n",
            "│   ├── \u001b[01;34mMessidor2\u001b[0m\n",
            "│   │   └── \u001b[01;34mmessidor-2\u001b[0m\n",
            "│   ├── \u001b[01;34mODIR5K\u001b[0m\n",
            "│   │   ├── \u001b[01;34mdatasets\u001b[0m\n",
            "│   │   ├── \u001b[01;34moutput\u001b[0m\n",
            "│   │   └── \u001b[01;34msplit_data\u001b[0m\n",
            "│   └── \u001b[01;34mRFMiD\u001b[0m\n",
            "│       ├── \u001b[01;34mEvaluation_Set\u001b[0m\n",
            "│       ├── \u001b[01;34mTest_Set\u001b[0m\n",
            "│       └── \u001b[01;34mTraining_Set\u001b[0m\n",
            "├── \u001b[01;34mensemble\u001b[0m\n",
            "├── \u001b[01;34mfederated\u001b[0m\n",
            "├── \u001b[01;34mmodels\u001b[0m\n",
            "│   └── \u001b[01;34modir\u001b[0m\n",
            "├── \u001b[01;34modir_b4_highacc\u001b[0m\n",
            "├── \u001b[01;34moutputs\u001b[0m\n",
            "├── \u001b[01;34mpreprocessing\u001b[0m\n",
            "│   ├── \u001b[01;34mAPTOS\u001b[0m\n",
            "│   ├── \u001b[01;34mCHASEDB1\u001b[0m\n",
            "│   ├── \u001b[01;34mDRIVE\u001b[0m\n",
            "│   ├── \u001b[01;34mMessidor2\u001b[0m\n",
            "│   ├── \u001b[01;34mODIR5K\u001b[0m\n",
            "│   └── \u001b[01;34mRFMiD\u001b[0m\n",
            "├── \u001b[01;34mreports\u001b[0m\n",
            "├── \u001b[01;34mresults\u001b[0m\n",
            "└── \u001b[01;34msegmentation\u001b[0m\n",
            "\n",
            "36 directories\n"
          ]
        }
      ],
      "source": [
        "!tree \"/content/drive/MyDrive/retinalnewproject\" -L 3 -d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JdjWiVonTRK",
        "outputId": "87f8bfda-03e1-4358-fd76-842e6495b03f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 125081 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_2.0.2-1_amd64.deb ...\n",
            "Unpacking tree (2.0.2-1) ...\n",
            "Setting up tree (2.0.2-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get install tree -qq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfhIEIdYph59",
        "outputId": "cf32ec38-6bed-45f5-ce10-aee3ee2c6795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Classes found: ['train_images']\n",
            "Training densenet121 on device: cpu\n",
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "100%|██████████| 30.8M/30.8M [00:00<00:00, 135MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [10:30<00:00, 10.33s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0000, Acc: 1.0000, F1: 1.0000\n",
            "\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [09:24<00:00,  9.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0000, Acc: 1.0000, F1: 1.0000\n",
            "\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [09:58<00:00,  9.81s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0000, Acc: 1.0000, F1: 1.0000\n",
            "\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [09:33<00:00,  9.40s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0000, Acc: 1.0000, F1: 1.0000\n",
            "\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [09:50<00:00,  9.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0000, Acc: 1.0000, F1: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Accuracy: 1.0000, F1: 1.0000\n",
            "Model saved to /content/drive/MyDrive/retinalnewproject/results/densenet121_APTOS.pth\n",
            "densenet121 => Accuracy: 1.0000, F1: 1.0000\n",
            "Training densenet169 on device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet169_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet169_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet169-b2777c0a.pth\" to /root/.cache/torch/hub/checkpoints/densenet169-b2777c0a.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 54.7M/54.7M [00:00<00:00, 149MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [11:09<00:00, 10.97s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0000, Acc: 1.0000, F1: 1.0000\n",
            "\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [11:19<00:00, 11.14s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0000, Acc: 1.0000, F1: 1.0000\n",
            "\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [11:22<00:00, 11.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0000, Acc: 1.0000, F1: 1.0000\n",
            "\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [11:29<00:00, 11.30s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0000, Acc: 1.0000, F1: 1.0000\n",
            "\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [11:22<00:00, 11.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0000, Acc: 1.0000, F1: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Accuracy: 1.0000, F1: 1.0000\n",
            "Model saved to /content/drive/MyDrive/retinalnewproject/results/densenet169_APTOS.pth\n",
            "densenet169 => Accuracy: 1.0000, F1: 1.0000\n",
            "Training densenet201 on device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /root/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 77.4M/77.4M [00:00<00:00, 136MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [14:04<00:00, 13.84s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0000, Acc: 1.0000, F1: 1.0000\n",
            "\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [14:16<00:00, 14.05s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0000, Acc: 1.0000, F1: 1.0000\n",
            "\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 93%|█████████▎| 57/61 [13:19<00:53, 13.42s/it]"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# DenseNet Training for APTOS Dataset\n",
        "# Path setup: /content/drive/MyDrive/retinalnewproject/datasets/APTOS\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Paths\n",
        "# ------------------------------------------------------------\n",
        "BASE_DIR = \"/content/drive/MyDrive/retinalnewproject\"\n",
        "DATA_DIR = os.path.join(BASE_DIR, \"datasets\", \"APTOS\")\n",
        "TRAIN_DIR = os.path.join(DATA_DIR, \"train_images\")\n",
        "TRAIN_DIR = os.path.join(DATA_DIR, \"train_images\", \"train_images\")\n",
        "VAL_DIR   = os.path.join(DATA_DIR, \"val_images\", \"val_images\")\n",
        "TEST_DIR  = os.path.join(DATA_DIR, \"test_images\", \"test_images\")\n",
        "\n",
        "VAL_DIR = os.path.join(DATA_DIR, \"val_images\")\n",
        "TEST_DIR = os.path.join(DATA_DIR, \"test_images\")\n",
        "RESULTS_DIR = os.path.join(BASE_DIR, \"results\")\n",
        "\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Data transforms\n",
        "# ------------------------------------------------------------\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Datasets and Dataloaders\n",
        "# ------------------------------------------------------------\n",
        "image_datasets = {\n",
        "    'train': datasets.ImageFolder(TRAIN_DIR, transform=data_transforms['train']),\n",
        "    'val': datasets.ImageFolder(VAL_DIR, transform=data_transforms['val'])\n",
        "}\n",
        "\n",
        "dataloaders = {\n",
        "    x: DataLoader(image_datasets[x], batch_size=16, shuffle=True, num_workers=2)\n",
        "    for x in ['train', 'val']\n",
        "}\n",
        "\n",
        "num_classes = len(image_datasets['train'].classes)\n",
        "print(f\"Classes found: {image_datasets['train'].classes}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# DenseNet Model Loader\n",
        "# ------------------------------------------------------------\n",
        "from torchvision.models import densenet121, densenet169, densenet201\n",
        "\n",
        "def get_densenet(model_name, num_classes):\n",
        "    if model_name == 'densenet121':\n",
        "        model = densenet121(pretrained=True)\n",
        "    elif model_name == 'densenet169':\n",
        "        model = densenet169(pretrained=True)\n",
        "    elif model_name == 'densenet201':\n",
        "        model = densenet201(pretrained=True)\n",
        "    else:\n",
        "        raise ValueError(\"Choose from: densenet121, densenet169, densenet201\")\n",
        "\n",
        "    model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Training Function\n",
        "# ------------------------------------------------------------\n",
        "def train_model(model_name='densenet121', epochs=5, lr=1e-4):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Training {model_name} on device: {device}\")\n",
        "\n",
        "    model = get_densenet(model_name, num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        preds, labels = [], []\n",
        "\n",
        "        for inputs, targets in tqdm(dataloaders['train']):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            preds.extend(outputs.argmax(1).cpu().numpy())\n",
        "            labels.extend(targets.cpu().numpy())\n",
        "\n",
        "        acc = accuracy_score(labels, preds)\n",
        "        f1 = f1_score(labels, preds, average='macro')\n",
        "        print(f\"Train Loss: {train_loss/len(dataloaders['train']):.4f}, Acc: {acc:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # Validation\n",
        "    # --------------------------------------------------------\n",
        "    model.eval()\n",
        "    val_preds, val_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloaders['val']:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            val_preds.extend(outputs.argmax(1).cpu().numpy())\n",
        "            val_labels.extend(targets.cpu().numpy())\n",
        "\n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
        "    print(f\"\\nValidation Accuracy: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
        "\n",
        "    # Save model\n",
        "    save_path = os.path.join(RESULTS_DIR, f\"{model_name}_APTOS.pth\")\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"Model saved to {save_path}\")\n",
        "\n",
        "    return val_acc, val_f1\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Run training for each DenseNet model\n",
        "# ------------------------------------------------------------\n",
        "for model_name in ['densenet121', 'densenet169', 'densenet201']:\n",
        "    acc, f1 = train_model(model_name, epochs=5)\n",
        "    print(f\"{model_name} => Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MnMAS0kFuXM",
        "outputId": "72639dcb-9166-42eb-820f-e1600344ee50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📂 Folder: /content/drive/MyDrive/retinalnewproject/datasets/APTOS\n",
            "   Subfolders: ['train_images', 'test_images', '.ipynb_checkpoints']\n",
            "   Example files: ['test.csv', 'train_1.csv', 'valid.csv', 'train_1.gsheet']\n",
            "\n",
            "📂 Folder: /content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images\n",
            "   Subfolders: ['val_images', 'train_images']\n",
            "\n",
            "📂 Folder: /content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images/val_images\n",
            "   Subfolders: ['val_images']\n",
            "\n",
            "📂 Folder: /content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images/val_images/val_images\n",
            "   Example files: ['00cc2b75cddd.png', '0212dd31f623.png', '0232dfea7547.png', '0180bfa26c0b.png', '014508ccb9cb.png']\n",
            "\n",
            "📂 Folder: /content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images/train_images\n",
            "   Example files: ['1b862fb6f65d.png', '1b8701231c8f.png', '1b3647865779.png', '1b398c0494d1.png', '1d37f1c8b6d8.png']\n",
            "\n",
            "📂 Folder: /content/drive/MyDrive/retinalnewproject/datasets/APTOS/test_images\n",
            "   Subfolders: ['test_images']\n",
            "\n",
            "📂 Folder: /content/drive/MyDrive/retinalnewproject/datasets/APTOS/test_images/test_images\n",
            "   Example files: ['e50b0174690d.png', 'e5197d77ec68.png', 'e529c5757d64.png', 'e4dcca36ceb4.png', 'e594c19e2e1d.png']\n",
            "\n",
            "📂 Folder: /content/drive/MyDrive/retinalnewproject/datasets/APTOS/.ipynb_checkpoints\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Base path to your APTOS dataset\n",
        "aptos_path = \"/content/drive/MyDrive/retinalnewproject/datasets/APTOS\"\n",
        "\n",
        "for root, dirs, files in os.walk(aptos_path):\n",
        "    print(f\"\\n📂 Folder: {root}\")\n",
        "    if dirs:\n",
        "        print(\"   Subfolders:\", dirs)\n",
        "    if files[:5]:\n",
        "        print(\"   Example files:\", files[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BysyWOZKGupQ",
        "outputId": "dd6d2d38-f8e3-4a94-8181-b774b1d113da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ CSVs loaded successfully\n",
            "Train samples: 2930  | Val samples: 366\n",
            "Columns: ['id_code', 'diagnosis']\n",
            "Detected 5 unique classes\n",
            "\n",
            "🚀 Training densenet121 on cpu\n",
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [11:02<00:00, 10.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.5300 | Acc: 0.8821 | F1: 0.3299\n",
            "\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [10:18<00:00, 10.14s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.2372 | Acc: 0.9338 | F1: 0.5553\n",
            "\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [10:36<00:00, 10.44s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.1652 | Acc: 0.9514 | F1: 0.6143\n",
            "\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [10:19<00:00, 10.16s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.1339 | Acc: 0.9607 | F1: 0.7129\n",
            "\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [10:28<00:00, 10.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0910 | Acc: 0.9762 | F1: 0.8329\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Validation Accuracy: 0.9464, F1: 0.5689\n",
            "💾 Model saved to /content/drive/MyDrive/retinalnewproject/results/densenet121_APTOS.pth\n",
            "densenet121 => Accuracy: 94.64%, F1: 56.89%\n",
            "\n",
            "🚀 Training densenet169 on cpu\n",
            "Downloading: \"https://download.pytorch.org/models/densenet169-b2777c0a.pth\" to /root/.cache/torch/hub/checkpoints/densenet169-b2777c0a.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 54.7M/54.7M [00:00<00:00, 139MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [12:26<00:00, 12.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.5965 | Acc: 0.8273 | F1: 0.3616\n",
            "\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [12:32<00:00, 12.33s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.2227 | Acc: 0.9349 | F1: 0.5832\n",
            "\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [12:30<00:00, 12.30s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.1848 | Acc: 0.9338 | F1: 0.5153\n",
            "\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [12:33<00:00, 12.36s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.1488 | Acc: 0.9452 | F1: 0.7251\n",
            "\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [12:26<00:00, 12.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.1143 | Acc: 0.9576 | F1: 0.7288\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Validation Accuracy: 0.9643, F1: 0.5500\n",
            "💾 Model saved to /content/drive/MyDrive/retinalnewproject/results/densenet169_APTOS.pth\n",
            "densenet169 => Accuracy: 96.43%, F1: 55.00%\n",
            "\n",
            "🚀 Training densenet201 on cpu\n",
            "Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /root/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 77.4M/77.4M [00:00<00:00, 84.9MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [15:39<00:00, 15.40s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.6524 | Acc: 0.8056 | F1: 0.3411\n",
            "\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [16:09<00:00, 15.89s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.2326 | Acc: 0.9276 | F1: 0.4915\n",
            "\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [15:46<00:00, 15.52s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.1659 | Acc: 0.9483 | F1: 0.6379\n",
            "\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [15:48<00:00, 15.55s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.1314 | Acc: 0.9617 | F1: 0.7719\n",
            "\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [15:24<00:00, 15.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0913 | Acc: 0.9741 | F1: 0.8305\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Validation Accuracy: 0.9643, F1: 0.5083\n",
            "💾 Model saved to /content/drive/MyDrive/retinalnewproject/results/densenet201_APTOS.pth\n",
            "densenet201 => Accuracy: 96.43%, F1: 50.83%\n",
            "\n",
            "📊 Summary of Results:\n",
            "densenet121: Accuracy=94.64% | F1=56.89%\n",
            "densenet169: Accuracy=96.43% | F1=55.00%\n",
            "densenet201: Accuracy=96.43% | F1=50.83%\n",
            "\n",
            "🔍 Running inference on test images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [00:36<00:00,  4.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Test predictions saved to /content/drive/MyDrive/retinalnewproject/results/APTOS_test_predictions.csv\n",
            "\n",
            "🎯 Sample predictions:\n",
            "              image  predicted_diagnosis\n",
            "0  e50b0174690d.png                    0\n",
            "1  e5197d77ec68.png                    0\n",
            "2  e529c5757d64.png                    0\n",
            "3  e4dcca36ceb4.png                    0\n",
            "4  e594c19e2e1d.png                    0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# DenseNet Training + Evaluation for APTOS Dataset\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# PATHS (specific to your structure)\n",
        "# ------------------------------------------------------------\n",
        "BASE_DIR = \"/content/drive/MyDrive/retinalnewproject\"\n",
        "DATA_DIR = os.path.join(BASE_DIR, \"datasets\", \"APTOS\")\n",
        "\n",
        "# Corrected paths based on file structure\n",
        "TRAIN_IMG_DIR = os.path.join(DATA_DIR, \"train_images\", \"train_images\")\n",
        "VAL_IMG_DIR   = os.path.join(DATA_DIR, \"train_images\", \"val_images\", \"val_images\")\n",
        "TEST_IMG_DIR  = os.path.join(DATA_DIR, \"test_images\", \"test_images\")\n",
        "RESULTS_DIR   = os.path.join(BASE_DIR, \"results\")\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# LOAD CSV LABELS\n",
        "# ------------------------------------------------------------\n",
        "train_csv = pd.read_csv(os.path.join(DATA_DIR, \"train_1.csv\"))\n",
        "val_csv   = pd.read_csv(os.path.join(DATA_DIR, \"valid.csv\"))\n",
        "\n",
        "print(\"✅ CSVs loaded successfully\")\n",
        "print(\"Train samples:\", len(train_csv), \" | Val samples:\", len(val_csv))\n",
        "print(\"Columns:\", list(train_csv.columns))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# TRANSFORMS\n",
        "# ------------------------------------------------------------\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# CUSTOM DATASET CLASS\n",
        "# ------------------------------------------------------------\n",
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.data = csv_file\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Filter out missing images\n",
        "        for _, row in self.data.iterrows():\n",
        "            img_id = row['id_code']\n",
        "            img_name = f\"{img_id}.png\" # Assuming .png extension based on directory listing\n",
        "            img_path = os.path.join(self.img_dir, img_name)\n",
        "\n",
        "            if os.path.exists(img_path):\n",
        "                self.image_paths.append(img_path)\n",
        "                self.labels.append(int(row['diagnosis']))\n",
        "            # else:\n",
        "                # print(f\"Warning: Image file not found for id {img_id} at {img_path}. Skipping.\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# DATASETS & DATALOADERS\n",
        "# ------------------------------------------------------------\n",
        "train_dataset = APTOSDataset(train_csv, TRAIN_IMG_DIR, transform_train)\n",
        "val_dataset   = APTOSDataset(val_csv, VAL_IMG_DIR, transform_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "num_classes = len(train_csv['diagnosis'].unique())\n",
        "print(f\"Detected {num_classes} unique classes\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# DENSENET MODELS (with updated 'weights' API)\n",
        "# ------------------------------------------------------------\n",
        "from torchvision.models import (\n",
        "    densenet121, densenet169, densenet201,\n",
        "    DenseNet121_Weights, DenseNet169_Weights, DenseNet201_Weights\n",
        ")\n",
        "\n",
        "def get_densenet(model_name, num_classes):\n",
        "    if model_name == 'densenet121':\n",
        "        model = densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1)\n",
        "    elif model_name == 'densenet169':\n",
        "        model = densenet169(weights=DenseNet169_Weights.IMAGENET1K_V1)\n",
        "    elif model_name == 'densenet201':\n",
        "        model = densenet201(weights=DenseNet201_Weights.IMAGENET1K_V1)\n",
        "    else:\n",
        "        raise ValueError(\"Choose from: densenet121, densenet169, densenet201\")\n",
        "\n",
        "    model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# TRAINING FUNCTION\n",
        "# ------------------------------------------------------------\n",
        "def train_model(model_name='densenet121', epochs=5, lr=1e-4):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"\\n🚀 Training {model_name} on {device}\")\n",
        "\n",
        "    model = get_densenet(model_name, num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        model.train()\n",
        "        running_loss, preds, labels = 0, [], []\n",
        "\n",
        "        for inputs, targets in tqdm(train_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            preds.extend(outputs.argmax(1).cpu().numpy())\n",
        "            labels.extend(targets.cpu().numpy())\n",
        "\n",
        "        train_acc = accuracy_score(labels, preds)\n",
        "        train_f1 = f1_score(labels, preds, average='macro')\n",
        "        print(f\"Train Loss: {running_loss/len(train_loader):.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_preds, val_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            val_preds.extend(outputs.argmax(1).cpu().numpy())\n",
        "            val_labels.extend(targets.cpu().numpy())\n",
        "\n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
        "    print(f\"\\n✅ Validation Accuracy: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
        "\n",
        "    save_path = os.path.join(RESULTS_DIR, f\"{model_name}_APTOS.pth\")\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"💾 Model saved to {save_path}\")\n",
        "\n",
        "    return model, val_acc, val_f1\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# RUN ALL DENSENET VARIANTS\n",
        "# ------------------------------------------------------------\n",
        "results_summary = []\n",
        "for model_name in ['densenet121', 'densenet169', 'densenet201']:\n",
        "    model, acc, f1 = train_model(model_name, epochs=5)\n",
        "    results_summary.append((model_name, acc, f1))\n",
        "    print(f\"{model_name} => Accuracy: {acc*100:.2f}%, F1: {f1*100:.2f}%\")\n",
        "\n",
        "print(\"\\n📊 Summary of Results:\")\n",
        "for name, acc, f1 in results_summary:\n",
        "    print(f\"{name}: Accuracy={acc*100:.2f}% | F1={f1*100:.2f}%\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# EVALUATION ON TEST IMAGES (PREDICTION ONLY)\n",
        "# ------------------------------------------------------------\n",
        "class APTOSTestDataset(Dataset):\n",
        "    def __init__(self, img_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.images = [f for f in os.listdir(img_dir) if f.endswith('.png')]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, img_name\n",
        "\n",
        "# Load best model for testing (DenseNet121 by default)\n",
        "best_model_path = os.path.join(RESULTS_DIR, \"densenet121_APTOS.pth\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = get_densenet(\"densenet121\", num_classes).to(device)\n",
        "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "test_dataset = APTOSTestDataset(TEST_IMG_DIR, transform_val)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "predictions = []\n",
        "filenames = []\n",
        "\n",
        "print(\"\\n🔍 Running inference on test images...\")\n",
        "with torch.no_grad():\n",
        "    for inputs, names in tqdm(test_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        preds = outputs.argmax(1).cpu().numpy()\n",
        "        predictions.extend(preds)\n",
        "        filenames.extend(names)\n",
        "\n",
        "# Save test predictions\n",
        "test_results = pd.DataFrame({'image': filenames, 'predicted_diagnosis': predictions})\n",
        "test_csv_path = os.path.join(RESULTS_DIR, \"APTOS_test_predictions.csv\")\n",
        "test_results.to_csv(test_csv_path, index=False)\n",
        "print(f\"✅ Test predictions saved to {test_csv_path}\")\n",
        "\n",
        "print(\"\\n🎯 Sample predictions:\")\n",
        "print(test_results.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfb31af5",
        "outputId": "8ca74e96-fdf2-4d15-a0af-a38bd3bf6978"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking for file: /content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images/train_images/5ce5eeaf757a.png\n",
            "❌ File does NOT exist.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "TRAIN_IMG_DIR = \"/content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images/train_images\"\n",
        "missing_image_id = \"5ce5eeaf757a\"\n",
        "missing_image_name = f\"{missing_image_id}.png\"\n",
        "missing_image_path = os.path.join(TRAIN_IMG_DIR, missing_image_name)\n",
        "\n",
        "print(f\"Checking for file: {missing_image_path}\")\n",
        "if os.path.exists(missing_image_path):\n",
        "    print(\"✅ File exists!\")\n",
        "else:\n",
        "    print(\"❌ File does NOT exist.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df8c8cab",
        "outputId": "7c21628b-0947-425f-9be8-76bc93385682"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contents of the training image directory:\n",
            "['1b862fb6f65d.png', '1b8701231c8f.png', '1b3647865779.png', '1b398c0494d1.png', '1d37f1c8b6d8.png', '1d2472849dce.png', '1d11794057ff.png', '1cb6961d141c.png', '1c13a1483f4a.png', '1d3e9b939732.png', '1ca35d483772.png', '1c7a013eeba7.png', '1ca91751be4d.png', '1db0393cdbc1.png', '1da4a17c18c9.png', '1f9ccda4ddf2.png', '1f31701dd61b.png', '1ffaa51a6245.png', '1f63d44d9e3c.png', '1faf8664816c.png']\n",
            "... and 967 total files\n",
            "\n",
            "Sample 'id_code' values from the training CSV:\n",
            "['1ae8c165fd53', '1b329a127307', '1b32e1d775ea', '1b3647865779', '1b398c0494d1', '1b4625877527', '1b495ac025b7', '1b862fb6f65d', '1b8701231c8f', '1b8ad0afe9fb', '1bb0ddfe753a', '1bea04b2bb2d', '1bf30c84bbad', '1c0cf251b426', '1c0e5dd1b14c', '1c13a1483f4a', '1c3a6b4449e9', '1c47815f4a6b', '1c4d87baaffc', '1c4f3aa4df06']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming TRAIN_IMG_DIR and train_csv are already defined from the previous cell\n",
        "# TRAIN_IMG_DIR = os.path.join(DATA_DIR, \"train_images\", \"train_images\")\n",
        "# train_csv = pd.read_csv(os.path.join(DATA_DIR, \"train_1.csv\"))\n",
        "\n",
        "print(\"Contents of the training image directory:\")\n",
        "try:\n",
        "    print(os.listdir(TRAIN_IMG_DIR)[:20]) # Print the first 20 files\n",
        "    print(f\"... and {len(os.listdir(TRAIN_IMG_DIR))} total files\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Training image directory not found at {TRAIN_IMG_DIR}\")\n",
        "\n",
        "print(\"\\nSample 'id_code' values from the training CSV:\")\n",
        "print(train_csv['id_code'].head(20).tolist()) # Print the first 20 id_codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "sSxw0PMB2Njh",
        "outputId": "6b1c41c4-c409-456f-a4fe-6772b9ad20d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ CSVs loaded successfully\n",
            "Train samples: 2930  | Val samples: 366\n",
            "Columns: ['id_code', 'diagnosis']\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class counts: {0: 1434, 1: 300, 2: 808, 3: 154, 4: 234}\n",
            "Dataset initialized with 967 valid images. Skipped 1963 missing entries.\n",
            "Dataset initialized with 112 valid images. Skipped 254 missing entries.\n",
            "Detected 5 unique classes\n",
            "Class weights (based on filtered data): tensor([0.0274, 0.6780, 0.2696, 3.1963, 0.8287])\n",
            "\n",
            "🚀 Training densenet121 (Balanced) on cpu\n",
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 31%|███       | 19/61 [03:28<06:54,  9.86s/it]"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# DenseNet Training for APTOS Dataset (Balanced using Class Weights)\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# PATHS (your structure)\n",
        "# ------------------------------------------------------------\n",
        "BASE_DIR = \"/content/drive/MyDrive/retinalnewproject\"\n",
        "DATA_DIR = os.path.join(BASE_DIR, \"datasets\", \"APTOS\")\n",
        "\n",
        "TRAIN_IMG_DIR = os.path.join(DATA_DIR, \"train_images\", \"train_images\")\n",
        "VAL_IMG_DIR   = os.path.join(DATA_DIR, \"train_images\", \"val_images\", \"val_images\")\n",
        "TEST_IMG_DIR  = os.path.join(DATA_DIR, \"test_images\", \"test_images\")\n",
        "RESULTS_DIR   = os.path.join(BASE_DIR, \"results\")\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# LOAD CSV LABELS\n",
        "# ------------------------------------------------------------\n",
        "train_csv = pd.read_csv(os.path.join(DATA_DIR, \"train_1.csv\"))\n",
        "val_csv   = pd.read_csv(os.path.join(DATA_DIR, \"valid.csv\"))\n",
        "\n",
        "print(\"✅ CSVs loaded successfully\")\n",
        "print(\"Train samples:\", len(train_csv), \" | Val samples:\", len(val_csv))\n",
        "print(\"Columns:\", list(train_csv.columns))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# VISUALIZE CLASS IMBALANCE\n",
        "# ------------------------------------------------------------\n",
        "class_counts = train_csv['diagnosis'].value_counts().sort_index()\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(class_counts.index.astype(str), class_counts.values, color='teal')\n",
        "plt.title(\"Class Distribution in Training Data\")\n",
        "plt.xlabel(\"Diagnosis Class\")\n",
        "plt.ylabel(\"Number of Images\")\n",
        "plt.show()\n",
        "print(\"Class counts:\", class_counts.to_dict())\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# TRANSFORMS\n",
        "# ------------------------------------------------------------\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# CUSTOM DATASET CLASS\n",
        "# ------------------------------------------------------------\n",
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.data = csv_file\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Build a mapping of all images present in the directory for fast lookup\n",
        "        # Use os.path.splitext(f)[0] to get the base filename without extension\n",
        "        self.available_images = {os.path.splitext(f)[0]: os.path.join(root, f)\n",
        "                                 for root, _, files in os.walk(img_dir)\n",
        "                                 for f in files if f.endswith('.png')} # Assuming only PNGs\n",
        "\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        missing_count = 0\n",
        "\n",
        "        # Filter out missing images based on the available_images map\n",
        "        for _, row in self.data.iterrows():\n",
        "            img_id = str(row['id_code']) # Ensure id_code is string\n",
        "            label = int(row['diagnosis'])\n",
        "\n",
        "            if img_id in self.available_images:\n",
        "                self.image_paths.append(self.available_images[img_id])\n",
        "                self.labels.append(label)\n",
        "            else:\n",
        "                missing_count += 1\n",
        "                # print(f\"Warning: Image file not found for id {img_id} in {img_dir}. Skipping.\")\n",
        "\n",
        "        print(f\"Dataset initialized with {len(self.image_paths)} valid images. Skipped {missing_count} missing entries.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# DATALOADERS\n",
        "# ------------------------------------------------------------\n",
        "train_dataset = APTOSDataset(train_csv, TRAIN_IMG_DIR, transform_train)\n",
        "val_dataset   = APTOSDataset(val_csv, VAL_IMG_DIR, transform_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "num_classes = len(train_csv['diagnosis'].unique())\n",
        "print(f\"Detected {num_classes} unique classes\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# COMPUTE CLASS WEIGHTS\n",
        "# ------------------------------------------------------------\n",
        "# Recalculate class counts based on the *filtered* training data\n",
        "filtered_train_counts = pd.Series(train_dataset.labels).value_counts().sort_index()\n",
        "counts = filtered_train_counts.values\n",
        "class_weights = 1. / torch.tensor(counts, dtype=torch.float)\n",
        "class_weights = class_weights / class_weights.sum() * len(counts)\n",
        "print(\"Class weights (based on filtered data):\", class_weights)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# DENSENET MODELS\n",
        "# ------------------------------------------------------------\n",
        "from torchvision.models import (\n",
        "    densenet121, densenet169, densenet201,\n",
        "    DenseNet121_Weights, DenseNet169_Weights, DenseNet201_Weights\n",
        ")\n",
        "\n",
        "def get_densenet(model_name, num_classes):\n",
        "    if model_name == 'densenet121':\n",
        "        model = densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1)\n",
        "    elif model_name == 'densenet169':\n",
        "        model = densenet169(weights=DenseNet169_Weights.IMAGENET1K_V1)\n",
        "    elif model_name == 'densenet201':\n",
        "        model = densenet201(weights=DenseNet201_Weights.IMAGENET1K_V1)\n",
        "    else:\n",
        "        raise ValueError(\"Choose from: densenet121, densenet169, densenet201\")\n",
        "\n",
        "    model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# TRAINING FUNCTION (Balanced)\n",
        "# ------------------------------------------------------------\n",
        "def train_model_balanced(model_name='densenet121', epochs=5, lr=1e-4):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"\\n🚀 Training {model_name} (Balanced) on {device}\")\n",
        "\n",
        "    model = get_densenet(model_name, num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        model.train()\n",
        "        running_loss, preds, labels = 0, [], []\n",
        "\n",
        "        for inputs, targets in tqdm(train_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            preds.extend(outputs.argmax(1).cpu().numpy())\n",
        "            labels.extend(targets.cpu().numpy())\n",
        "\n",
        "        train_acc = accuracy_score(labels, preds)\n",
        "        train_f1 = f1_score(labels, preds, average='macro')\n",
        "        print(f\"Train Loss: {running_loss/len(train_loader):.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_preds, val_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            val_preds.extend(outputs.argmax(1).cpu().numpy())\n",
        "            val_labels.extend(targets.cpu().numpy())\n",
        "\n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
        "    print(f\"\\n✅ Validation Accuracy: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
        "\n",
        "    save_path = os.path.join(RESULTS_DIR, f\"{model_name}_APTOS_balanced.pth\")\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"💾 Model saved to {save_path}\")\n",
        "\n",
        "    return model, val_acc, val_f1\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# TRAIN ALL DENSENET MODELS\n",
        "# ------------------------------------------------------------\n",
        "results_summary = []\n",
        "for model_name in ['densenet121', 'densenet169', 'densenet201']:\n",
        "    model, acc, f1 = train_model_balanced(model_name, epochs=5)\n",
        "    results_summary.append((model_name, acc, f1))\n",
        "    print(f\"{model_name} => Accuracy: {acc*100:.2f}%, F1: {f1*100:.2f}%\")\n",
        "\n",
        "print(\"\\n📊 Summary of Results:\")\n",
        "for name, acc, f1 in results_summary:\n",
        "    print(f\"{name}: Accuracy={acc*100:.2f}% | F1={f1*100:.2f}%\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# TEST EVALUATION (PREDICTION ONLY)\n",
        "# ------------------------------------------------------------\n",
        "class APTOSTestDataset(Dataset):\n",
        "    def __init__(self, img_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        # Build a mapping of all images present in the directory for fast lookup\n",
        "        self.available_images = {os.path.splitext(f)[0]: os.path.join(root, f)\n",
        "                                 for root, _, files in os.walk(img_dir)\n",
        "                                 for f in files if f.endswith('.png')} # Assuming only PNGs\n",
        "\n",
        "        self.image_paths = []\n",
        "        self.image_names = [] # Store names for the output CSV\n",
        "\n",
        "        # Filter out missing images for the test set as well\n",
        "        # Test CSV is not loaded here, so we just use available images\n",
        "        for base_name, full_path in self.available_images.items():\n",
        "             self.image_paths.append(full_path)\n",
        "             self.image_names.append(os.path.basename(full_path))\n",
        "\n",
        "\n",
        "        print(f\"Test Dataset initialized with {len(self.image_paths)} images.\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        img_name = self.image_names[idx] # Get the original filename\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, img_name # Return image and name\n",
        "\n",
        "\n",
        "# Load best model (DenseNet121) for testing\n",
        "best_model_path = os.path.join(RESULTS_DIR, \"densenet121_APTOS_balanced.pth\") # Assuming balanced model is best\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = get_densenet(\"densenet121\", num_classes).to(device)\n",
        "# Check if the model file exists before loading\n",
        "if os.path.exists(best_model_path):\n",
        "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "    print(f\"✅ Loaded model from {best_model_path}\")\n",
        "else:\n",
        "    print(f\"❌ Model file not found at {best_model_path}. Cannot run test inference.\")\n",
        "    # You might want to exit or handle this case appropriately\n",
        "    # For now, we'll just print and skip test inference.\n",
        "    model = None # Set model to None to skip inference\n",
        "\n",
        "if model is not None:\n",
        "    model.eval()\n",
        "\n",
        "    test_dataset = APTOSTestDataset(TEST_IMG_DIR, transform_val)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "    predictions, filenames = [], []\n",
        "\n",
        "    print(\"\\n🔍 Running inference on test images...\")\n",
        "    with torch.no_grad():\n",
        "        for inputs, names in tqdm(test_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            preds = outputs.argmax(1).cpu().numpy()\n",
        "            predictions.extend(preds)\n",
        "            filenames.extend(names)\n",
        "\n",
        "    # Save results\n",
        "    test_results = pd.DataFrame({'image': filenames, 'predicted_diagnosis': predictions})\n",
        "    csv_path = os.path.join(RESULTS_DIR, \"APTOS_test_predictions_balanced.csv\")\n",
        "    test_results.to_csv(csv_path, index=False)\n",
        "    print(f\"✅ Test predictions saved to {csv_path}\")\n",
        "    print(\"\\n🎯 Sample predictions:\")\n",
        "    print(test_results.head())\n",
        "else:\n",
        "    print(\"\\nSkipping test inference due to missing model file.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1EoNUmNhFaYC_s5ZR3wZtbObUsCYA2RbO",
      "authorship_tag": "ABX9TyMSn7bfzJIzXriqyKr8s5CZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}