{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCQBjuah/qrbKwNBEbGzVA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/israinamdar493-ops/retinalprojectsizz4/blob/main/finalproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Go inside your Drive project folder\n",
        "%cd /content/drive/MyDrive/retinalnewproject\n",
        "\n",
        "# (Optional) check the structure\n",
        "!ls -R | head -40   # shows first 40 lines of folder tree\n",
        "\n",
        "# Install all dependencies\n",
        "!pip install torch torchvision timm albumentations pandas scikit-learn xgboost opencv-python matplotlib\n",
        "\n",
        "# >>> RUN YOUR SCRIPT (Phase-1 demo or real data) <<<\n",
        "# Replace with the actual file you want to run, for example:\n",
        "!python src/phase1_demo.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9r_at-dWvTp",
        "outputId": "27c1ad4c-6f2f-45ac-95fe-e5e56911e0ce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/retinalnewproject\n",
            ".:\n",
            "datasets\n",
            "ensemble\n",
            "federated\n",
            "outputs\n",
            "preprocessing\n",
            "reports\n",
            "results\n",
            "segmentation\n",
            "\n",
            "./datasets:\n",
            "APTOS\n",
            "CHASEDB1\n",
            "DRIVE\n",
            "Messidor2\n",
            "ODIR5K\n",
            "RFMiD\n",
            "\n",
            "./datasets/APTOS:\n",
            "test.csv\n",
            "test_images\n",
            "train_1.csv\n",
            "train_images\n",
            "valid.csv\n",
            "val_images\n",
            "\n",
            "./datasets/APTOS/test_images:\n",
            "test_images\n",
            "\n",
            "./datasets/APTOS/test_images/test_images:\n",
            "e4dcca36ceb4.png\n",
            "e50b0174690d.png\n",
            "e5197d77ec68.png\n",
            "e529c5757d64.png\n",
            "e594c19e2e1d.png\n",
            "e5de79795c1d.png\n",
            "e60e4edb3ca9.png\n",
            "e6552b7432b3.png\n",
            "e66855a5c583.png\n",
            "e68746d426b2.png\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.19)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.12/dist-packages (2.0.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.34.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.6.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from albumentations) (1.16.1)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.11.7)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.12/dist-packages (from albumentations) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.12/dist-packages (from albumentations) (4.12.0.88)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (3.12.6)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (6.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.1.9)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.8.3)\n",
            "python3: can't open file '/content/drive/MyDrive/retinalnewproject/src/phase1_demo.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save this cell as a .py file inside your Drive\n",
        "demo_code = \"\"\"\n",
        "import torch, torchvision\n",
        "import pandas as pd\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "print('✅ Demo running with Torch version:', torch.__version__)\n",
        "\n",
        "# Define dataset paths - These should match the variables defined in the notebook\n",
        "train_csv = \"/content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_1.csv\"\n",
        "train_dir = \"/content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images\"\n",
        "val_csv = \"/content/drive/MyDrive/retinalnewproject/datasets/APTOS/valid.csv\"\n",
        "val_dir = \"/content/drive/MyDrive/retinalnewproject/datasets/APTOS/val_images\"\n",
        "\n",
        "\n",
        "class RetinalDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.df.iloc[idx, 0] + '.png')\n",
        "        image = Image.open(img_name)\n",
        "        label = self.df.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Define a simple transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "try:\n",
        "    train_dataset = RetinalDataset(train_csv, train_dir, transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    print(f\"✅ Loaded training dataset with {len(train_dataset)} images.\")\n",
        "\n",
        "    val_dataset = RetinalDataset(val_csv, val_dir, transform)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "    print(f\"✅ Loaded validation dataset with {len(val_dataset)} images.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"❌ File not found error: {e}\")\n",
        "    print(\"Please ensure the dataset paths in the script are correct.\")\n",
        "    exit() # Exit the script if dataset loading fails\n",
        "\n",
        "# Fake training loop\n",
        "for epoch in range(1,4):\n",
        "    print(f'Epoch {epoch}: training...done!')\n",
        "print('✅ Finished demo training.')\n",
        "\"\"\"\n",
        "with open('/content/drive/MyDrive/retinalnewproject/phase1_demo.py', 'w') as f:\n",
        "    f.write(demo_code)\n",
        "\n",
        "print(\"✅ File created at /content/drive/MyDrive/retinalnewproject/phase1_demo.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PT_faPw3XTA-",
        "outputId": "5fdc5075-7b02-44b9-c812-35e9a6a2487e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ File created at /content/drive/MyDrive/retinalnewproject/phase1_demo.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/retinalnewproject/phase1_demo.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRr9JkE1XWOQ",
        "outputId": "ef882f6f-927c-4649-f4dd-8edd3d84aac4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Demo running with Torch version: 2.8.0+cu126\n",
            "Epoch 1: training...done!\n",
            "Epoch 2: training...done!\n",
            "Epoch 3: training...done!\n",
            "✅ Finished demo training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/retinalnewproject/phase1_demo.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WobtfNVTYLa-",
        "outputId": "ea172d0c-0a24-4770-fcd3-85e4d101c3aa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/retinalnewproject/phase1_demo.py\", line 58, in <module>\n",
            "    train_dataset = RetinalDataset(train_csv, train_dir, transform)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/retinalnewproject/phase1_demo.py\", line 18, in __init__\n",
            "    self.df = pd.read_csv(csv_file)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n",
            "    return _read(filepath_or_buffer, kwds)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 620, in _read\n",
            "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\n",
            "    self._engine = self._make_engine(f, self.engine)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 1880, in _make_engine\n",
            "    self.handles = get_handle(\n",
            "                   ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\", line 873, in get_handle\n",
            "    handle = open(\n",
            "             ^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/retinalnewproject/datasets/ODIR5K/train_1.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/retinalnewproject/datasets/APTOS\"\n",
        "\n",
        "train_csv = os.path.join(base_path, \"train_1.csv\")\n",
        "train_dir = os.path.join(base_path, \"train_images\")\n",
        "\n",
        "val_csv = os.path.join(base_path, \"valid.csv\")\n",
        "val_dir = os.path.join(base_path, \"val_images\")"
      ],
      "metadata": {
        "id": "5XZKjoiZY3gl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"/content/drive/MyDrive/retinalnewproject/datasets/ODIR5K\"\n"
      ],
      "metadata": {
        "id": "HvuJHCkOZF0N"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"/content/drive/MyDrive/retinalnewproject/datasets/RFMiD\"\n"
      ],
      "metadata": {
        "id": "aLw2aXv-ZJFS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# phase1_demo.py\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# -------------------------------\n",
        "# 0. Select Dataset\n",
        "# -------------------------------\n",
        "# Change this to \"APTOS\", \"ODIR5K\", or \"RFMiD\"\n",
        "DATASET_NAME = \"APTOS\"\n",
        "\n",
        "base_path = f\"/content/drive/MyDrive/retinalnewproject/datasets/{DATASET_NAME}\"\n",
        "\n",
        "train_csv = os.path.join(base_path, \"train_1.csv\")\n",
        "train_dir = os.path.join(base_path, \"train_images\")\n",
        "\n",
        "val_csv = os.path.join(base_path, \"valid.csv\")\n",
        "val_dir = os.path.join(base_path, \"val_images\")\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Dataset Class\n",
        "# -------------------------------\n",
        "class RetinalDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # map labels to integers if not numeric\n",
        "        if not np.issubdtype(self.df['label'].dtype, np.integer):\n",
        "            self.df['label'] = pd.Categorical(self.df['label']).codes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['image'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = torch.tensor(row['label'], dtype=torch.long)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Transformations\n",
        "# -------------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Load Datasets\n",
        "# -------------------------------\n",
        "train_dataset = RetinalDataset(train_csv, train_dir, transform)\n",
        "val_dataset   = RetinalDataset(val_csv, val_dir, transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Model\n",
        "# -------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = len(pd.unique(train_dataset.df['label']))\n",
        "model = models.efficientnet_b0(weights=None, num_classes=num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Training Loop\n",
        "# -------------------------------\n",
        "epochs = 3  # Increase for longer training\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Validation\n",
        "# -------------------------------\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "f1  = f1_score(all_labels, all_preds, average='weighted')\n",
        "print(f\"Validation Accuracy: {acc:.4f} | F1 Score: {f1:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 7. Save Model\n",
        "# -------------------------------\n",
        "results_path = \"/content/drive/MyDrive/retinalnewproject/results\"\n",
        "os.makedirs(results_path, exist_ok=True)\n",
        "torch.save(model.state_dict(), os.path.join(results_path, f\"{DATASET_NAME}_efficientnet_phase1.pth\"))\n",
        "print(f\"✅ Model saved to {results_path}/{DATASET_NAME}_efficientnet_phase1.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "YP3V4MhkZYNM",
        "outputId": "463b6496-9f69-4e70-9c1c-dc8b62cd4860"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'label'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'label'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1186909369.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# 3. Load Datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# -------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetinalDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0mval_dataset\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mRetinalDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1186909369.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, csv_file, img_dir, transform)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# map labels to integers if not numeric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'label'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_1.csv\")\n",
        "print(df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFN26710ZmZn",
        "outputId": "9277f830-3072-41a7-d5aa-83fb69e6a95d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id_code', 'diagnosis'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbaa2980",
        "outputId": "558c91ad-bdb7-4cc8-f16f-c11182502716"
      },
      "source": [
        "!python /content/drive/MyDrive/retinalnewproject/phase1_demo.py"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/retinalnewproject/phase1_demo.py\", line 89, in <module>\n",
            "    for images, labels in train_loader:\n",
            "                          ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 734, in __next__\n",
            "    data = self._next_data()\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 790, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "            ~~~~~~~~~~~~^^^^^\n",
            "  File \"/content/drive/MyDrive/retinalnewproject/phase1_demo.py\", line 47, in __getitem__\n",
            "    image = Image.open(img_path).convert('RGB')\n",
            "            ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/PIL/Image.py\", line 3513, in open\n",
            "    fp = builtins.open(filename, \"rb\")\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images/4e54ccfd49b2.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Original\n",
        "train_dir = os.path.join(base_path, \"train_images\")\n",
        "val_dir   = os.path.join(base_path, \"val_images\")\n",
        "\n",
        "# Corrected (include the extra 'test_images' folder)\n",
        "train_dir = os.path.join(base_path, \"train_images\", \"test_images\")\n",
        "val_dir   = os.path.join(base_path, \"val_images\", \"val_images\")  # check val_images folder similarly\n"
      ],
      "metadata": {
        "id": "vqNj2I15aQ77"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sXngSUnaThP",
        "outputId": "1f1dffd9-8c3f-4b76-e7e5-6a5764f954bd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/retinalnewproject/phase1_demo.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlSPqiDxbfq7",
        "outputId": "dd2be5cc-a20a-4075-d903-096ff636a9ee"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train folder: /content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images/train_images\n",
            "Val folder:   /content/drive/MyDrive/retinalnewproject/datasets/APTOS/val_images/val_images\n",
            "Test folder:  /content/drive/MyDrive/retinalnewproject/datasets/APTOS/test_images/test_images\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/retinalnewproject/phase1_demo.py\", line 118, in <module>\n",
            "    for images, labels in train_loader:\n",
            "                          ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 734, in __next__\n",
            "    data = self._next_data()\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 790, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "            ~~~~~~~~~~~~^^^^^\n",
            "  File \"/content/drive/MyDrive/retinalnewproject/phase1_demo.py\", line 74, in __getitem__\n",
            "    raise FileNotFoundError(f\"Image file not found for id {row['id_code']} in {self.img_dir}\")\n",
            "FileNotFoundError: Image file not found for id 1df0431bfa73 in /content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images/train_images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# phase1_demo.py\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# -------------------------------\n",
        "# 0. Dataset Paths\n",
        "# -------------------------------\n",
        "DATASET_NAME = \"APTOS\"\n",
        "base_path = f\"/content/drive/MyDrive/retinalnewproject/datasets/{DATASET_NAME}\"\n",
        "\n",
        "train_csv = os.path.join(base_path, \"train_1.csv\")\n",
        "val_csv   = os.path.join(base_path, \"valid.csv\")\n",
        "test_csv  = os.path.join(base_path, \"test.csv\")  # optional\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Recursive folder detection\n",
        "# -------------------------------\n",
        "def get_image_folder(base_folder):\n",
        "    \"\"\"Recursively find first folder containing image files.\"\"\"\n",
        "    for root, dirs, files in os.walk(base_folder):\n",
        "        img_files = [f for f in files if f.lower().endswith(('.png','.jpg','.jpeg'))]\n",
        "        if len(img_files) > 0:\n",
        "            return root\n",
        "    raise ValueError(f\"No image files found in {base_folder}\")\n",
        "\n",
        "train_dir = get_image_folder(os.path.join(base_path, \"train_images\"))\n",
        "val_dir   = get_image_folder(os.path.join(base_path, \"val_images\"))\n",
        "test_dir  = get_image_folder(os.path.join(base_path, \"test_images\"))\n",
        "\n",
        "print(f\"Train folder: {train_dir}\")\n",
        "print(f\"Val folder:   {val_dir}\")\n",
        "print(f\"Test folder:  {test_dir}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Dataset Class\n",
        "# -------------------------------\n",
        "class RetinalDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, label_col='diagnosis', transform=None):\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.label_col = label_col\n",
        "\n",
        "        self.df['id_code'] = self.df['id_code'].astype(str).str.strip()\n",
        "\n",
        "        # Map labels to integers if not numeric\n",
        "        if not np.issubdtype(self.df[self.label_col].dtype, np.integer):\n",
        "            self.df[self.label_col] = pd.Categorical(self.df[self.label_col]).codes\n",
        "\n",
        "        # Build mapping from id_code -> full image path\n",
        "        self.id_to_path = {}\n",
        "        for root, dirs, files in os.walk(self.img_dir):\n",
        "            for f in files:\n",
        "                name, ext = os.path.splitext(f)\n",
        "                if name in self.df['id_code'].values and ext.lower() in ['.png','.jpg','.jpeg']:\n",
        "                    self.id_to_path[name] = os.path.join(root, f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        id_code = row['id_code']\n",
        "        if id_code not in self.id_to_path:\n",
        "            raise FileNotFoundError(f\"Image file not found for id {id_code} in {self.img_dir}\")\n",
        "        img_path = self.id_to_path[id_code]\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = torch.tensor(row[self.label_col], dtype=torch.long)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Transformations\n",
        "# -------------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Load datasets\n",
        "# -------------------------------\n",
        "train_dataset = RetinalDataset(train_csv, train_dir, label_col='diagnosis', transform=transform)\n",
        "val_dataset   = RetinalDataset(val_csv, val_dir, label_col='diagnosis', transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Model\n",
        "# -------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = len(pd.unique(train_dataset.df['diagnosis']))\n",
        "model = models.efficientnet_b0(weights=None, num_classes=num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Training loop\n",
        "# -------------------------------\n",
        "epochs = 3  # increase for better accuracy\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 7. Validation\n",
        "# -------------------------------\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "f1  = f1_score(all_labels, all_preds, average='weighted')\n",
        "print(f\"Validation Accuracy: {acc:.4f} | F1 Score: {f1:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 8. Save model\n",
        "# -------------------------------\n",
        "results_path = \"/content/drive/MyDrive/retinalnewproject/results\"\n",
        "os.makedirs(results_path, exist_ok=True)\n",
        "torch.save(model.state_dict(), os.path.join(results_path, f\"{DATASET_NAME}_efficientnet_phase1.pth\"))\n",
        "print(f\"✅ Model saved to {results_path}/{DATASET_NAME}_efficientnet_phase1.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 931
        },
        "collapsed": true,
        "id": "0FKYA6ltewtM",
        "outputId": "85c9d27d-eccb-4848-a5fe-68dd714c9e26"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train folder: /content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images/train_images\n",
            "Val folder:   /content/drive/MyDrive/retinalnewproject/datasets/APTOS/val_images/val_images\n",
            "Test folder:  /content/drive/MyDrive/retinalnewproject/datasets/APTOS/test_images/test_images\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Image file not found for id 916ec976ff30 in /content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images/train_images",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2734753324.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2734753324.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mid_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id_code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mid_code\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_to_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Image file not found for id {id_code} in {self.img_dir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_to_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_code\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Image file not found for id 916ec976ff30 in /content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images/train_images"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!python /content/drive/MyDrive/retinalnewproject/phase1_demo.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPweMqesfRkU",
        "outputId": "582b7201-e0a5-4907-e6eb-bb7232a60db5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Train folder: /content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images/train_images\n",
            "Val folder:   /content/drive/MyDrive/retinalnewproject/datasets/APTOS/val_images/val_images\n",
            "Test folder:  /content/drive/MyDrive/retinalnewproject/datasets/APTOS/test_images/test_images\n",
            "Using 967 images from /content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images/train_images\n",
            "Using 112 images from /content/drive/MyDrive/retinalnewproject/datasets/APTOS/val_images/val_images\n",
            "Epoch 1/3 | Train Loss: 3.2950\n",
            "Epoch 2/3 | Train Loss: 1.2566\n",
            "Epoch 3/3 | Train Loss: 0.7312\n",
            "Validation Accuracy: 0.9018 | F1 Score: 0.8666\n",
            "✅ Model saved to /content/drive/MyDrive/retinalnewproject/results/APTOS_efficientnet_phase1.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Gradio if not installed\n",
        "!pip install gradio --quiet\n",
        "\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# -------------------\n",
        "# Settings\n",
        "# -------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "num_classes = 5  # change to your number of disease classes\n",
        "model_path = \"/content/drive/MyDrive/retinalnewproject/results/APTOS_efficientnet_phase1.pth\"\n",
        "\n",
        "# -------------------\n",
        "# Image preprocessing\n",
        "# -------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# -------------------\n",
        "# Load model\n",
        "# -------------------\n",
        "model = models.efficientnet_b0(weights=None, num_classes=num_classes)\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# -------------------\n",
        "# Grad-CAM function\n",
        "# -------------------\n",
        "def grad_cam(input_image, class_idx=None):\n",
        "    # Hook for gradients\n",
        "    gradients = []\n",
        "    def save_gradients(module, grad_input, grad_output):\n",
        "        gradients.append(grad_output[0])\n",
        "    target_layer = model.features[-1]\n",
        "    target_layer.register_backward_hook(save_gradients)\n",
        "\n",
        "    # Prepare image\n",
        "    input_tensor = transform(input_image).unsqueeze(0).to(device)\n",
        "    input_tensor.requires_grad = True\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(input_tensor)\n",
        "    if class_idx is None:\n",
        "        class_idx = torch.argmax(output, dim=1).item()\n",
        "\n",
        "    # Backward pass\n",
        "    model.zero_grad()\n",
        "    loss = output[0, class_idx]\n",
        "    loss.backward()\n",
        "\n",
        "    grads = gradients[0].cpu().data.numpy()[0]\n",
        "    fmap = target_layer(input_tensor).cpu().data.numpy()[0]\n",
        "\n",
        "    # Grad-CAM calculation\n",
        "    weights = np.mean(grads, axis=(1,2))\n",
        "    cam = np.zeros(fmap.shape[1:], dtype=np.float32)\n",
        "    for i, w in enumerate(weights):\n",
        "        cam += w * fmap[i]\n",
        "    cam = np.maximum(cam, 0)\n",
        "    cam = cam / cam.max()\n",
        "    cam = np.uint8(cam * 255)\n",
        "    cam = Image.fromarray(cam).resize(input_image.size, Image.ANTIALIAS)\n",
        "    return cam\n",
        "\n",
        "# -------------------\n",
        "# Prediction + Grad-CAM\n",
        "# -------------------\n",
        "def predict_with_gradcam(img):\n",
        "    # Predict\n",
        "    input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "    output = model(input_tensor)\n",
        "    pred_idx = torch.argmax(output, dim=1).item()\n",
        "\n",
        "    # Map class index to label (adjust this list to your dataset)\n",
        "    labels = ['Normal', 'Diabetic Retinopathy', 'Glaucoma', 'Cataract', 'Other']\n",
        "    pred_label = labels[pred_idx]\n",
        "\n",
        "    # Grad-CAM overlay\n",
        "    cam = grad_cam(img, class_idx=pred_idx)\n",
        "    img_with_cam = Image.blend(img.convert(\"RGB\"), cam.convert(\"RGB\"), alpha=0.5)\n",
        "\n",
        "    return pred_label, img_with_cam\n",
        "\n",
        "# -------------------\n",
        "# Launch Gradio app\n",
        "# -------------------\n",
        "interface = gr.Interface(\n",
        "    fn=predict_with_gradcam,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=[gr.Textbox(label=\"Predicted Disease\"), gr.Image(label=\"Grad-CAM Heatmap\")],\n",
        "    title=\"Retinal Disease Detection\",\n",
        "    description=\"Upload a fundus image to get predicted disease and Grad-CAM heatmap.\"\n",
        ")\n",
        "\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "qNnpkVYJkFLj",
        "outputId": "ad6305aa-64df-4d6f-a2d3-87f89a37b98a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f6c32cdd5511314805.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f6c32cdd5511314805.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gradio --quiet\n",
        "\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# -------------------\n",
        "# Settings\n",
        "# -------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "num_classes = 5\n",
        "model_path = \"/content/drive/MyDrive/retinalnewproject/results/APTOS_efficientnet_phase1.pth\"\n",
        "\n",
        "# -------------------\n",
        "# Preprocessing\n",
        "# -------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# -------------------\n",
        "# Load model\n",
        "# -------------------\n",
        "model = models.efficientnet_b0(weights=None, num_classes=num_classes)\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# -------------------\n",
        "# Grad-CAM function\n",
        "# -------------------\n",
        "def grad_cam(input_image, class_idx=None):\n",
        "    gradients = []\n",
        "\n",
        "    def save_gradients(module, grad_input, grad_output):\n",
        "        gradients.append(grad_output[0])\n",
        "\n",
        "    target_layer = model.features[-1]\n",
        "    target_layer.register_full_backward_hook(save_gradients)\n",
        "\n",
        "    input_tensor = transform(input_image).unsqueeze(0).to(device)\n",
        "    input_tensor.requires_grad = True\n",
        "    output = model(input_tensor)\n",
        "    if class_idx is None:\n",
        "        class_idx = torch.argmax(output, dim=1).item()\n",
        "    model.zero_grad()\n",
        "    loss = output[0, class_idx]\n",
        "    loss.backward()\n",
        "\n",
        "    grads = gradients[0].cpu().data.numpy()[0]\n",
        "    fmap = target_layer(input_tensor).cpu().data.numpy()[0]\n",
        "    weights = np.mean(grads, axis=(1, 2))\n",
        "    cam = np.zeros(fmap.shape[1:], dtype=np.float32)\n",
        "    for i, w in enumerate(weights):\n",
        "        cam += w * fmap[i]\n",
        "    cam = np.maximum(cam, 0)\n",
        "    cam = cam / (cam.max() + 1e-8)\n",
        "    cam = np.uint8(cam * 255)\n",
        "    cam = Image.fromarray(cam).resize(input_image.size, Image.ANTIALIAS).convert(\"RGB\")\n",
        "    return cam\n",
        "\n",
        "# -------------------\n",
        "# Prediction + Grad-CAM\n",
        "# -------------------\n",
        "labels = ['Normal', 'Diabetic Retinopathy', 'Glaucoma', 'Cataract', 'Other']\n",
        "\n",
        "def predict_multiple(file_paths):\n",
        "    preds = []\n",
        "    cams = []\n",
        "    if not isinstance(file_paths, list):\n",
        "        file_paths = [file_paths]\n",
        "\n",
        "    for f in file_paths:\n",
        "        img = Image.open(f).convert(\"RGB\")\n",
        "        input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "        output = model(input_tensor)\n",
        "        pred_idx = torch.argmax(output, dim=1).item()\n",
        "        pred_label = labels[pred_idx]\n",
        "        preds.append(f\"{os.path.basename(f)}: {pred_label}\")\n",
        "\n",
        "        cam = grad_cam(img, class_idx=pred_idx)\n",
        "        blended = Image.blend(img, cam, alpha=0.5)\n",
        "        cams.append(blended)\n",
        "\n",
        "    # Return predictions as a single string\n",
        "    return \"\\n\".join(preds), cams\n",
        "\n",
        "# -------------------\n",
        "# Gradio Interface\n",
        "# -------------------\n",
        "interface = gr.Interface(\n",
        "    fn=predict_multiple,\n",
        "    inputs=gr.File(file_types=[\".png\", \".jpg\", \".jpeg\"], file_count=\"multiple\", label=\"Upload Fundus Images\"),\n",
        "    outputs=[gr.Textbox(label=\"Predicted Diseases\"), gr.Gallery(label=\"Grad-CAM Heatmaps\")],\n",
        "    title=\"Retinal Disease Detection\",\n",
        "    description=\"Upload one or more fundus images to get predictions and Grad-CAM heatmaps.\"\n",
        ")\n",
        "\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "t8NKpU8Qk5Hu",
        "outputId": "9c6a54df-227e-4f12-c4ec-5199a0cb45a1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5799619c6f7b159e42.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5799619c6f7b159e42.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Base dataset folder\n",
        "dataset_base = \"/content/drive/MyDrive/retinalnewproject/datasets\"\n",
        "\n",
        "# List all datasets\n",
        "for dataset_name in os.listdir(dataset_base):\n",
        "    dataset_path = os.path.join(dataset_base, dataset_name)\n",
        "    if os.path.isdir(dataset_path):\n",
        "        print(f\"\\nDataset: {dataset_name}\")\n",
        "\n",
        "        # List CSV files\n",
        "        csv_files = [f for f in os.listdir(dataset_path) if f.endswith(\".csv\")]\n",
        "        print(\"  CSV files:\")\n",
        "        for f in csv_files:\n",
        "            print(f\"    {os.path.join(dataset_path, f)}\")\n",
        "\n",
        "        # List image folders and first few images\n",
        "        img_folders = [f for f in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, f))]\n",
        "        print(\"  Image folders:\")\n",
        "        for f in img_folders:\n",
        "            img_folder_path = os.path.join(dataset_path, f)\n",
        "            imgs = os.listdir(img_folder_path)\n",
        "            print(f\"    {img_folder_path} ({len(imgs)} files)\")\n",
        "            print(f\"      Sample images: {imgs[:5]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NqOuYjJb_RJ",
        "outputId": "a7438321-c211-4fa2-a99c-359137f52368"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset: DRIVE\n",
            "  CSV files:\n",
            "  Image folders:\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/DRIVE/DRIVE (2 files)\n",
            "      Sample images: ['training', 'test']\n",
            "\n",
            "Dataset: .ipynb_checkpoints\n",
            "  CSV files:\n",
            "  Image folders:\n",
            "\n",
            "Dataset: APTOS\n",
            "  CSV files:\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/APTOS/test.csv\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_1.csv\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/APTOS/valid.csv\n",
            "  Image folders:\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/APTOS/val_images (1 files)\n",
            "      Sample images: ['val_images']\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images (1 files)\n",
            "      Sample images: ['train_images']\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/APTOS/test_images (1 files)\n",
            "      Sample images: ['test_images']\n",
            "\n",
            "Dataset: CHASEDB1\n",
            "  CSV files:\n",
            "  Image folders:\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/CHASEDB1/Images (28 files)\n",
            "      Sample images: ['Image_12L.jpg', 'Image_14R.jpg', 'Image_06R.jpg', 'Image_04R.jpg', 'Image_10L.jpg']\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/CHASEDB1/Masks (28 files)\n",
            "      Sample images: ['Image_03L_1stHO.png', 'Image_12R_1stHO.png', 'Image_05R_1stHO.png', 'Image_13R_1stHO.png', 'Image_14L_1stHO.png']\n",
            "\n",
            "Dataset: Messidor2\n",
            "  CSV files:\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/Messidor2/messidor_data.csv\n",
            "  Image folders:\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/Messidor2/messidor-2 (1 files)\n",
            "      Sample images: ['messidor-2']\n",
            "\n",
            "Dataset: ODIR5K\n",
            "  CSV files:\n",
            "  Image folders:\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/ODIR5K/datasets (8 files)\n",
            "      Sample images: ['normal', 'ageDegeneration', 'hypertension', 'myopia', 'others']\n",
            "\n",
            "Dataset: RFMiD\n",
            "  CSV files:\n",
            "  Image folders:\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/RFMiD/Test_Set (1 files)\n",
            "      Sample images: ['Test_Set']\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/RFMiD/Evaluation_Set (1 files)\n",
            "      Sample images: ['Evaluation_Set']\n",
            "    /content/drive/MyDrive/retinalnewproject/datasets/RFMiD/Training_Set (1 files)\n",
            "      Sample images: ['Training_Set']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_actual_image_folder(base_folder):\n",
        "    subfolders = [f for f in os.listdir(base_folder) if os.path.isdir(os.path.join(base_folder, f))]\n",
        "    if len(subfolders) == 0:\n",
        "        raise ValueError(f\"No subfolders found in {base_folder}\")\n",
        "    return os.path.join(base_folder, subfolders[0])\n",
        "\n",
        "train_dir = get_actual_image_folder(\"/content/drive/MyDrive/retinalnewproject/datasets/APTOS/train_images\")\n",
        "val_dir   = get_actual_image_folder(\"/content/drive/MyDrive/retinalnewproject/datasets/APTOS/val_images\")\n",
        "test_dir  = get_actual_image_folder(\"/content/drive/MyDrive/retinalnewproject/datasets/APTOS/test_images\")\n"
      ],
      "metadata": {
        "id": "7QWBWlzgcoSR"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}