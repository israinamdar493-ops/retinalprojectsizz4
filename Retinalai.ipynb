{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/israinamdar493-ops/retinalprojectsizz4/blob/main/Retinalai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "base_path = \"/content/drive/MyDrive/retinalnewproject\"\n",
        "print(os.listdir(base_path))\n"
      ],
      "metadata": {
        "id": "bRRs5VvgNjYm",
        "outputId": "0f06f1c1-b6dd-496e-90ad-40a806b75604",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['datasets', 'outputs', 'reports', 'preprocessing', 'segmentation', 'federated', 'ensemble', 'results']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import densenet121, DenseNet121_Weights\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Training on:\", device)\n",
        "\n",
        "# Load pretrained DenseNet121\n",
        "model = densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1)\n",
        "model.classifier = nn.Linear(model.classifier.in_features, 3)  # 3 classes\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, preds = outputs.max(1)\n",
        "\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "num_epochs = 10\n",
        "best_acc = 0\n",
        "save_path = \"/content/drive/MyDrive/retinalnewproject/results/odir3_densenet121.pth\"\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for images, labels in pbar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        pbar.set_postfix({\"loss\": running_loss / len(train_loader)})\n",
        "\n",
        "    val_acc = evaluate(model, val_loader)\n",
        "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(\"âœ” Best model saved!\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "print(\"Best validation accuracy:\", best_acc)\n",
        "print(\"Model saved at:\", save_path)"
      ],
      "metadata": {
        "id": "GRdTqp4geSrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers accelerate transformers --quiet\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from diffusers import DDPMScheduler, UNet2DModel\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from PIL import Image\n"
      ],
      "metadata": {
        "id": "ReiLHLiueS-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Run this single cell in Colab ===\n",
        "# installs\n",
        "!pip install -q diffusers accelerate transformers\n",
        "\n",
        "# imports\n",
        "import os, sys, random, math\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from diffusers import DDPMScheduler, UNet2DModel\n",
        "\n",
        "# === SETTINGS ===\n",
        "image_size = 256\n",
        "batch_size = 8            # keep small for Colab GPU; increase if you have >12GB GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# paths (uses your existing split_data folders)\n",
        "BASE_ODIR = \"/content/drive/MyDrive/retinalnewproject/datasets/ODIR5K/split_data\"\n",
        "TRAIN_DIR = os.path.join(BASE_ODIR, \"train\")\n",
        "VAL_DIR   = os.path.join(BASE_ODIR, \"val\")\n",
        "\n",
        "RESULTS = \"/content/drive/MyDrive/retinalnewproject/generated/odir_ddpm_256\"\n",
        "os.makedirs(RESULTS, exist_ok=True)\n",
        "\n",
        "print(\"image_size:\", image_size, \"batch_size:\", batch_size, \"device:\", device)\n",
        "print(\"TRAIN_DIR exists:\", os.path.exists(TRAIN_DIR))\n",
        "print(\"VAL_DIR exists:\", os.path.exists(VAL_DIR))\n",
        "print(\"Results folder:\", RESULTS)\n",
        "\n",
        "# === Dataset that loads only the three folders (diabetes, glaucoma, ageDegeneration) ===\n",
        "CLASS_MAP = {\"diabetes\":0, \"glaucoma\":1, \"ageDegeneration\":2}\n",
        "class DDPMDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.samples = []\n",
        "        self.transform = transform\n",
        "        for name in CLASS_MAP.keys():\n",
        "            folder = os.path.join(root_dir, name)\n",
        "            if not os.path.exists(folder):\n",
        "                continue\n",
        "            for f in os.listdir(folder):\n",
        "                if f.lower().endswith((\".png\",\".jpg\",\".jpeg\")):\n",
        "                    self.samples.append(os.path.join(folder,f))\n",
        "        print(\"Found\", len(self.samples), \"images in\", root_dir)\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        p = self.samples[idx]\n",
        "        img = Image.open(p).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "# transforms for diffusion: scale to [-1,1]\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.CenterCrop((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])   # maps [0,1] -> [-1,1]\n",
        "])\n",
        "\n",
        "train_ds = DDPMDataset(TRAIN_DIR, transform=transform)\n",
        "val_ds   = DDPMDataset(VAL_DIR, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# quick sanity check: show one batch shape and a grid saved to DRIVE\n",
        "batch = next(iter(train_loader))\n",
        "print(\"Batch tensor shape:\", batch.shape)   # expected [B,3,H,W]\n",
        "\n",
        "# save a small grid to RESULTS for quick visual check\n",
        "grid = utils.make_grid((batch[:8]*0.5+0.5).clamp(0,1), nrow=4)  # undo [-1,1] to [0,1]\n",
        "utils.save_image(grid, os.path.join(RESULTS, \"sample_batch_grid.png\"))\n",
        "print(\"Saved sample grid to:\", os.path.join(RESULTS, \"sample_batch_grid.png\"))\n",
        "\n",
        "# check diffusers UNet available (not building/training yet)\n",
        "unet = UNet2DModel(sample_size=image_size, in_channels=3, out_channels=3, layers_per_block=2)\n",
        "print(\"UNet model created (parameters):\", sum(p.numel() for p in unet.parameters()) )\n",
        "print(\"Ready for next step: build scheduler + small training loop.\")\n"
      ],
      "metadata": {
        "id": "elT73h8jeTId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Diffusion Scheduler\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "timesteps = 1000\n",
        "beta_start = 0.0001\n",
        "beta_end = 0.02\n",
        "\n",
        "betas = torch.linspace(beta_start, beta_end, timesteps).to(device)\n",
        "alphas = 1 - betas\n",
        "alpha_hat = torch.cumprod(alphas, dim=0)  # cumulative product\n",
        "\n",
        "print(\"Scheduler ready - timesteps:\", timesteps)\n"
      ],
      "metadata": {
        "id": "cpYtx9VxeTSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Quick DDPM training + sampling (run 200 steps) ===\n",
        "import torch, os, math, random\n",
        "from tqdm import trange\n",
        "from torchvision import utils\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# reuse variables created earlier in your session:\n",
        "# - unet (UNet2DModel instance)\n",
        "# - train_loader\n",
        "# - betas, alphas, alpha_hat (scheduler tensors)\n",
        "# - RESULTS path\n",
        "\n",
        "# Safety checks / defaults if not present\n",
        "try:\n",
        "    unet\n",
        "except NameError:\n",
        "    raise RuntimeError(\"UNet object not found in the session. Run the UNet creation cell first.\")\n",
        "\n",
        "if 'RESULTS' not in globals():\n",
        "    RESULTS = \"/content/drive/MyDrive/retinalnewproject/generated/odir_ddpm_256\"\n",
        "os.makedirs(RESULTS, exist_ok=True)\n",
        "\n",
        "# training hyperparams (fast demo)\n",
        "num_train_steps = 200       # total gradient steps (small demo)\n",
        "save_every = 50\n",
        "sample_every = 50\n",
        "lr = 2e-4\n",
        "\n",
        "optimizer = torch.optim.Adam(unet.parameters(), lr=lr)\n",
        "\n",
        "T = betas.shape[0]\n",
        "print(f\"Using T={T} diffusion steps. Running {num_train_steps} training steps.\")\n",
        "\n",
        "global_step = 0\n",
        "unet.to(device)\n",
        "unet.train()\n",
        "\n",
        "# helper: q_sample (adds noise according to alpha_hat at selected timesteps)\n",
        "def q_sample(x_start, t, noise):\n",
        "    # x_start: [B,3,H,W], t: [B] long tensor with values in [0..T-1], noise same shape\n",
        "    a_hat = alpha_hat[t].view(-1,1,1,1).to(device)   # (B,1,1,1)\n",
        "    sqrt_a_hat = torch.sqrt(a_hat)\n",
        "    sqrt_one_minus = torch.sqrt(1. - a_hat)\n",
        "    return sqrt_a_hat * x_start + sqrt_one_minus * noise\n",
        "\n",
        "# training loop (iterate data loader repeatedly until num_train_steps)\n",
        "loader_iter = iter(train_loader)\n",
        "pbar = trange(num_train_steps, desc=\"DDPM steps\")\n",
        "for step in pbar:\n",
        "    try:\n",
        "        batch = next(loader_iter)\n",
        "    except StopIteration:\n",
        "        loader_iter = iter(train_loader)\n",
        "        batch = next(loader_iter)\n",
        "    x0 = batch.to(device)           # images already normalized to [-1,1]\n",
        "    bsz = x0.shape[0]\n",
        "\n",
        "    # sample random timesteps for each sample\n",
        "    t = torch.randint(0, T, (bsz,), device=device).long()\n",
        "\n",
        "    # sample noise and create x_t\n",
        "    noise = torch.randn_like(x0).to(device)\n",
        "    x_t = q_sample(x0, t, noise)\n",
        "\n",
        "    # forward through unet -> predict noise\n",
        "    out = unet(x_t).sample if hasattr(unet(x_t), \"sample\") else unet(x_t)\n",
        "    pred_noise = out\n",
        "\n",
        "    loss = torch.nn.functional.mse_loss(pred_noise, noise)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    global_step += 1\n",
        "    pbar.set_postfix({\"loss\": f\"{loss.item():.6f}\", \"step\": global_step})\n",
        "\n",
        "    # save checkpoint\n",
        "    if global_step % save_every == 0 or global_step == num_train_steps:\n",
        "        ckpt_path = os.path.join(RESULTS, f\"ddpm_unet_step{global_step}.pt\")\n",
        "        torch.save({'step': global_step, 'model_state': unet.state_dict(), 'opt_state': optimizer.state_dict()}, ckpt_path)\n",
        "        print(f\"\\nSaved checkpoint: {ckpt_path}\")\n",
        "\n",
        "    # sampling (simple ancestral sampling)\n",
        "    if global_step % sample_every == 0 or global_step == num_train_steps:\n",
        "        unet.eval()\n",
        "        n_samples = 8\n",
        "        x = torch.randn(n_samples, 3, image_size, image_size, device=device)  # start from noise\n",
        "        with torch.no_grad():\n",
        "            for step_t in reversed(range(T)):\n",
        "                t_batch = torch.full((n_samples,), step_t, device=device, dtype=torch.long)\n",
        "                out = unet(x).sample if hasattr(unet(x), \"sample\") else unet(x)\n",
        "                pred_noise = out\n",
        "\n",
        "                alpha_t = alphas[step_t].to(device)\n",
        "                alpha_cum_t = alpha_hat[step_t].to(device)\n",
        "                beta_t = betas[step_t].to(device)\n",
        "\n",
        "                if step_t > 0:\n",
        "                    noise_term = torch.randn_like(x)\n",
        "                else:\n",
        "                    noise_term = torch.zeros_like(x)\n",
        "\n",
        "                # simplified posterior update\n",
        "                x = (1.0 / torch.sqrt(alpha_t)) * (x - ((1 - alpha_t) / torch.sqrt(1 - alpha_cum_t)) * pred_noise) + torch.sqrt(beta_t) * noise_term\n",
        "\n",
        "            samples = (x.clamp(-1,1) + 1) / 2.0   # to [0,1]\n",
        "            grid = utils.make_grid(samples.cpu(), nrow=4)\n",
        "            sample_path = os.path.join(RESULTS, f\"sample_step{global_step}.png\")\n",
        "            utils.save_image(grid, sample_path)\n",
        "            print(f\"Saved sample grid: {sample_path}\")\n",
        "        unet.train()\n",
        "\n",
        "print(\"\\nDDPM quick run finished.\")\n",
        "print(\"All samples and checkpoints saved to:\", RESULTS)\n"
      ],
      "metadata": {
        "id": "FR50gAnDeTab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bk_DgDFCfpL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rfwcn3n4fpR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZxV1BNYJeT2g"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}